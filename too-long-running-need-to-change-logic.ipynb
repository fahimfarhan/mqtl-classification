{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3bf23c56",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-04-09T04:16:32.737300Z",
          "iopub.status.busy": "2025-04-09T04:16:32.736835Z",
          "iopub.status.idle": "2025-04-09T04:16:34.048833Z",
          "shell.execute_reply": "2025-04-09T04:16:34.048119Z"
        },
        "papermill": {
          "duration": 1.31779,
          "end_time": "2025-04-09T04:16:34.050536",
          "exception": false,
          "start_time": "2025-04-09T04:16:32.732746",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "46ee3a66",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:16:34.057556Z",
          "iopub.status.busy": "2025-04-09T04:16:34.057142Z",
          "iopub.status.idle": "2025-04-09T04:16:54.624053Z",
          "shell.execute_reply": "2025-04-09T04:16:54.623197Z"
        },
        "papermill": {
          "duration": 20.572407,
          "end_time": "2025-04-09T04:16:54.626232",
          "exception": false,
          "start_time": "2025-04-09T04:16:34.053825",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\r\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\r\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\r\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\r\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\r\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\r\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\r\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\r\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\r\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\r\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\r\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\r\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\r\n",
            "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\r\n",
            "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\r\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\r\n",
            "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\r\n",
            "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\r\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\r\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\r\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\r\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\r\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\r\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\r\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\r\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\r\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\r\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\r\n",
            "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\r\n",
            "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\r\n",
            "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\r\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
            "\u001b[?25h"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing collected packages: evaluate\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed evaluate-0.4.3\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting huggingface\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing collected packages: huggingface\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed huggingface-0.0.1\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.10/dist-packages (0.29.0)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: huggingface-hub 0.29.0 does not provide the extra 'hf-xet'\u001b[0m\u001b[33m\r\n",
            "\u001b[0mRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (3.17.0)\r\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (2024.12.0)\r\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (24.2)\r\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\r\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\r\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\r\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (4.12.2)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing collected packages: python-dotenv\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed python-dotenv-1.1.0\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\r\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\r\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\r\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\r\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\r\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\r\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\r\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a2)\r\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\r\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\r\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\r\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\r\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\r\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\r\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\r\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\r\n",
            "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.29.0)\r\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\r\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\r\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\r\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\r\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\r\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate\n",
        "!pip install huggingface\n",
        "!pip install huggingface_hub[hf_xet]  # sth to do with offline support\n",
        "!pip install python-dotenv\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "76c3b9d9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:16:54.635933Z",
          "iopub.status.busy": "2025-04-09T04:16:54.635672Z",
          "iopub.status.idle": "2025-04-09T04:17:32.566151Z",
          "shell.execute_reply": "2025-04-09T04:17:32.565248Z"
        },
        "papermill": {
          "duration": 37.939096,
          "end_time": "2025-04-09T04:17:32.570177",
          "exception": false,
          "start_time": "2025-04-09T04:16:54.631081",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "import dependencies completed\n"
          ]
        }
      ],
      "source": [
        "\"\"\" import dependencies \"\"\"\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import evaluate\n",
        "import huggingface_hub\n",
        "from huggingface_hub import HfApi\n",
        "import numpy as np\n",
        "import wandb\n",
        "from datasets import load_dataset, Dataset\n",
        "from dotenv import load_dotenv\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
        "from torch.utils.data import IterableDataset\n",
        "from transformers import BertTokenizer, BatchEncoding, AutoTokenizer, \\\n",
        "    AutoModelForSequenceClassification, AutoConfig, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "import torch\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # hyena dna requires this\n",
        "print(\"import dependencies completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c3658c2c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:17:32.578597Z",
          "iopub.status.busy": "2025-04-09T04:17:32.578378Z",
          "iopub.status.idle": "2025-04-09T04:17:32.832750Z",
          "shell.execute_reply": "2025-04-09T04:17:32.831921Z"
        },
        "papermill": {
          "duration": 0.25985,
          "end_time": "2025-04-09T04:17:32.833923",
          "exception": false,
          "start_time": "2025-04-09T04:17:32.574073",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reconcile my code with kaggle\n"
          ]
        }
      ],
      "source": [
        "\"\"\" load_env does not work in kaggle. a simple hack to reconcile the issue \"\"\"\n",
        "import os\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "user_secrets = UserSecretsClient()\n",
        "os.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "os.environ[\"WAND_DB_API_KEY\"] = user_secrets.get_secret(\"WAND_DB_API_KEY\")\n",
        "\n",
        "print(\"Reconcile my code with kaggle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4c445c51",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:17:32.843212Z",
          "iopub.status.busy": "2025-04-09T04:17:32.842937Z",
          "iopub.status.idle": "2025-04-09T04:17:35.226284Z",
          "shell.execute_reply": "2025-04-09T04:17:35.225443Z"
        },
        "papermill": {
          "duration": 2.389584,
          "end_time": "2025-04-09T04:17:35.227727",
          "exception": false,
          "start_time": "2025-04-09T04:17:32.838143",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "359dd58cdf1c42fbb59ceedbb538bbad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20a1f9b8272b4ec283e18f1197c54c28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13a025aef05a4bfabfd5593c7722a601",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/9.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddf5d9be859a43718d82cfe1c87c8d1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.56k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b161fdba1b6a4abdae02e87ac6ff8c22",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.38k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "init common completed\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Common codes \"\"\"\n",
        "# some colors for visual convenience\n",
        "red = \"\\u001b[31m\"\n",
        "green = \"\\u001b[32m\"\n",
        "yellow = \"\\u001b[33m\"\n",
        "blue = \"\\u001b[34m\"\n",
        "\n",
        "timber = logging.getLogger()\n",
        "# logging.basicConfig(level=logging.DEBUG)\n",
        "logging.basicConfig(level=logging.INFO)  # change to level=logging.DEBUG to print more logs...\n",
        "\n",
        "\n",
        "def getDynamicGpuDevice():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")  # For NVIDIA GPUs\n",
        "    elif torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")  # For Apple Silicon Macs\n",
        "    else:\n",
        "        return torch.device(\"cpu\")   # Fallback to CPU\n",
        "\n",
        "def getDynamicBatchSize():\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0).lower()\n",
        "        vramGiB = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert to GB\n",
        "\n",
        "        if \"a100\" in gpu_name:   # A100 (40GB+ VRAM)\n",
        "            batch_size = 128\n",
        "        elif \"v100\" in gpu_name:  # V100 (16GB/32GB VRAM)\n",
        "            batch_size = 64 if vramGiB >= 32 else 32\n",
        "        elif \"p100\" in gpu_name:  # P100 (16GB VRAM)\n",
        "            batch_size = 32\n",
        "        elif \"t4\" in gpu_name:    # Tesla T4 (16GB VRAM, common in Colab/Kaggle)\n",
        "            batch_size = 32  # Maybe try 64 if no OOM\n",
        "        elif \"rtx 3090\" in gpu_name or vramGiB >= 24:  # RTX 3090 (24GB VRAM)\n",
        "            batch_size = 64\n",
        "        elif vramGiB >= 16:   # Any other 16GB+ VRAM GPUs\n",
        "            batch_size = 32\n",
        "        elif vramGiB >= 8:    # 8GB VRAM GPUs (e.g., RTX 2080, 3060, etc.)\n",
        "            batch_size = 16\n",
        "        elif vramGiB >= 6:    # 6GB VRAM GPUs (e.g., RTX 2060)\n",
        "            batch_size = 8\n",
        "        else:\n",
        "            batch_size = 4  # Safe fallback for smaller GPUs\n",
        "    else:\n",
        "        batch_size = 4  # CPU mode, keep it small\n",
        "\n",
        "    return batch_size\n",
        "\n",
        "def getGpuName():\n",
        "    gpu_name = torch.cuda.get_device_name(0).lower()\n",
        "    return gpu_name\n",
        "\n",
        "# for hyenaDna. its tokenizer can process longer sequences...\n",
        "def sequenceEncodePlusForHyenaDna(\n",
        "    tokenizer: BertTokenizer,\n",
        "    sequence: str,\n",
        "    label: int\n",
        ") -> BatchEncoding:\n",
        "    input_ids = tokenizer(sequence)[\"input_ids\"]\n",
        "    input_ids: torch.Tensor = torch.Tensor(input_ids)\n",
        "    label_tensor = torch.tensor(label)\n",
        "    encoded_map: dict = {\n",
        "        \"input_ids\": input_ids.long(),\n",
        "        # \"attention_mask\": attention_mask.int(),    # hyenaDNA does not have attention layer\n",
        "        \"labels\": label_tensor\n",
        "    }\n",
        "\n",
        "    batchEncodingDict: BatchEncoding = BatchEncoding(encoded_map)\n",
        "    return batchEncodingDict\n",
        "\n",
        "# for dnaBert. it cannot process longer sequences...\n",
        "def sequenceEncodePlusWithSplitting(\n",
        "        tokenizer: BertTokenizer,\n",
        "        sequence: str,\n",
        "        label: int\n",
        ") -> BatchEncoding:\n",
        "    max_size = 512\n",
        "\n",
        "    tempMap: BatchEncoding = tokenizer.encode_plus(\n",
        "        sequence,\n",
        "        add_special_tokens=False,  # we'll add the special tokens manually in the for loop below\n",
        "        return_attention_mask=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    someInputIds1xN = tempMap[\"input_ids\"]  # shape = 1xN , N = sequence length\n",
        "    someMasks1xN = tempMap[\"attention_mask\"]\n",
        "    inputIdsList = list(someInputIds1xN[0].split(510))\n",
        "    masksList = list(someMasks1xN[0].split(510))\n",
        "\n",
        "    tmpLength: int = len(inputIdsList)\n",
        "\n",
        "    for i in range(0, tmpLength):\n",
        "        cls: torch.Tensor = torch.Tensor([101])\n",
        "        sep: torch.Tensor = torch.Tensor([102])\n",
        "\n",
        "        isTokenUnitTensor = torch.Tensor([1])\n",
        "\n",
        "        inputIdsList[i]: torch.Tensor = torch.cat([\n",
        "            cls,\n",
        "            inputIdsList[i],\n",
        "            sep\n",
        "        ])\n",
        "\n",
        "        masksList[i] = torch.cat([\n",
        "            isTokenUnitTensor,\n",
        "            masksList[i],\n",
        "            isTokenUnitTensor\n",
        "        ])\n",
        "\n",
        "\n",
        "        pad_len: int = max_size - inputIdsList[i].shape[0]\n",
        "        if pad_len > 0:\n",
        "            pad: torch.Tensor = torch.Tensor([0] * pad_len)\n",
        "\n",
        "            inputIdsList[i]: torch.Tensor = torch.cat([\n",
        "                inputIdsList[i],\n",
        "                pad\n",
        "            ])\n",
        "\n",
        "            masksList[i]: torch.Tensor = torch.cat([\n",
        "                masksList[i],\n",
        "                pad\n",
        "            ])\n",
        "\n",
        "\n",
        "    # so each item len = 512, and the last one may have some padding\n",
        "    input_ids: torch.Tensor = torch.stack(inputIdsList).squeeze()  # what's with this squeeze / unsqueeze thing? o.O\n",
        "    attention_mask: torch.Tensor = torch.stack(masksList)\n",
        "    label_tensor = torch.tensor(label)\n",
        "\n",
        "    # print(f\"{input_ids.shape = }\")\n",
        "\n",
        "    encoded_map: dict = {\n",
        "        \"input_ids\": input_ids.long(),\n",
        "        \"attention_mask\": attention_mask.int(),\n",
        "        \"labels\": label_tensor\n",
        "    }\n",
        "\n",
        "    batchEncodingDict: BatchEncoding = BatchEncoding(encoded_map)\n",
        "    return batchEncodingDict\n",
        "\n",
        "def sequenceEncodePlusCompact(\n",
        "        splitSequence: bool,\n",
        "        tokenizer: BertTokenizer,\n",
        "        sequence: str,\n",
        "        label: int\n",
        ") -> BatchEncoding:\n",
        "    if splitSequence:\n",
        "        return sequenceEncodePlusWithSplitting(tokenizer, sequence, label)\n",
        "    else:\n",
        "        return sequenceEncodePlusForHyenaDna(tokenizer, sequence, label)\n",
        "\n",
        "\n",
        "class PagingMQTLDataset(IterableDataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            someDataset: Dataset,\n",
        "            bertTokenizer: BertTokenizer,\n",
        "            seqLength: int,\n",
        "            splitSequenceRequired: bool\n",
        "        ):\n",
        "        self.someDataset = someDataset\n",
        "        self.bertTokenizer = bertTokenizer\n",
        "        self.seqLength = seqLength\n",
        "        self.splitSequenceRequired = splitSequenceRequired\n",
        "        pass\n",
        "\n",
        "    def __iter__(self):\n",
        "        for row in self.someDataset:\n",
        "            processed = self.preprocess(row)\n",
        "            if processed is not None:\n",
        "                yield processed\n",
        "\n",
        "    def preprocess(self, row: dict):\n",
        "        sequence = row[\"sequence\"]\n",
        "        label = row[\"label\"]\n",
        "\n",
        "        if len(sequence) != self.seqLength:\n",
        "            return None  # skip a few problematic rows\n",
        "\n",
        "        return sequenceEncodePlusCompact(self.splitSequenceRequired, self.bertTokenizer, sequence, label)\n",
        "\n",
        "def isMyLaptop() -> bool:\n",
        "    is_my_laptop = os.path.isfile(\"/home/gamegame/PycharmProjects/mqtl-classification/src/datageneration/dataset_4000_train_binned.csv\")\n",
        "    return is_my_laptop\n",
        "\n",
        "\n",
        "def signInToHuggingFaceAndWandbToUploadModelWeightsAndBiases():\n",
        "    # Load the .env file, but don't crash if it's not found (e.g., in Hugging Face Space)\n",
        "    try:\n",
        "        load_dotenv()  # Only useful on your laptop if .env exists\n",
        "        print(\".env file loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not load .env file. Exception: {e}\")\n",
        "\n",
        "    # Try to get the token from environment variables\n",
        "    try:\n",
        "        token = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "        if not token:\n",
        "            raise ValueError(\"HF_TOKEN not found. Make sure to set it in the environment variables or .env file.\")\n",
        "\n",
        "        # Log in to Hugging Face Hub\n",
        "        huggingface_hub.login(token)\n",
        "        print(\"Logged in to Hugging Face Hub successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Hugging Face login: {e}\")\n",
        "        # Handle the error appropriately (e.g., exit or retry)\n",
        "\n",
        "    # wand db login\n",
        "    try:\n",
        "        api_key = os.getenv(\"WAND_DB_API_KEY\")\n",
        "        timber.info(f\"{api_key = }\")\n",
        "\n",
        "        if not api_key:\n",
        "            raise ValueError(\n",
        "                \"WAND_DB_API_KEY not found. Make sure to set it in the environment variables or .env file.\")\n",
        "\n",
        "        # Log in to Hugging Face Hub\n",
        "        wandb.login(key=api_key)\n",
        "        print(\"Logged in to wand db successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during wand db Face login: {e}\")\n",
        "    pass\n",
        "\n",
        "def createPagingTrainValTestDatasets(tokenizer, window, splitSequenceRequired) -> (PagingMQTLDataset, PagingMQTLDataset, PagingMQTLDataset):\n",
        "    prefix = \"/home/gamegame/PycharmProjects/mqtl-classification/\"\n",
        "    data_files = {\n",
        "        # small samples\n",
        "        \"train_binned_200\": f\"{prefix}src/datageneration/dataset_200_train_binned.csv\",\n",
        "        \"validate_binned_200\": f\"{prefix}src/datageneration/dataset_200_validate_binned.csv\",\n",
        "        \"test_binned_200\": f\"{prefix}src/datageneration/dataset_200_test_binned.csv\",\n",
        "        # medium samples\n",
        "        \"train_binned_1000\": f\"{prefix}src/datageneration/dataset_1000_train_binned.csv\",\n",
        "        \"validate_binned_1000\": f\"{prefix}src/datageneration/dataset_1000_train_binned.csv\",\n",
        "        \"test_binned_1000\": f\"{prefix}src/datageneration/dataset_1000_train_binned.csv\",\n",
        "\n",
        "        # large samples\n",
        "        \"train_binned_4000\": f\"{prefix}src/datageneration/dataset_4000_train_binned.csv\",\n",
        "        \"validate_binned_4000\": f\"{prefix}src/datageneration/dataset_4000_train_binned.csv\",\n",
        "        \"test_binned_4000\": f\"{prefix}src/datageneration/dataset_4000_train_binned.csv\",\n",
        "    }\n",
        "\n",
        "    dataset_map = None\n",
        "    is_my_laptop = isMyLaptop()\n",
        "    if is_my_laptop:\n",
        "        dataset_map = load_dataset(\"csv\", data_files=data_files, streaming=True)\n",
        "    else:\n",
        "        dataset_map = load_dataset(\"fahimfarhan/mqtl-classification-datasets\", streaming=True)\n",
        "\n",
        "    train_dataset = PagingMQTLDataset(someDataset=dataset_map[f\"train_binned_{window}\"],\n",
        "                                    bertTokenizer=tokenizer,\n",
        "                                    seqLength=window,\n",
        "                                    splitSequenceRequired=splitSequenceRequired\n",
        "                                    )\n",
        "    val_dataset = PagingMQTLDataset(dataset_map[f\"validate_binned_{window}\"],\n",
        "                                  bertTokenizer=tokenizer,\n",
        "                                  seqLength=window,\n",
        "                                  splitSequenceRequired=splitSequenceRequired\n",
        "                                  )\n",
        "    test_dataset = PagingMQTLDataset(dataset_map[f\"test_binned_{window}\"],\n",
        "                                   bertTokenizer=tokenizer,\n",
        "                                   seqLength=window,\n",
        "                                   splitSequenceRequired=splitSequenceRequired\n",
        "                                   )\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "# Load metrics\n",
        "# global variables. bad practice...\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "roc_auc_metric = evaluate.load(\"roc_auc\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def computeMetricsUsingTorchEvaluate(args):\n",
        "    logits, labels = args\n",
        "    predictions = np.argmax(logits, axis=1)  # Get predicted class\n",
        "\n",
        "    positive_logits = logits[:, 1]  # Get positive class logits\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n",
        "    roc_auc = roc_auc_metric.compute(prediction_scores=positive_logits, references=labels)[\"roc_auc\"]  # using positive_logits repairs the error\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "# use sklearn cz torchmetrics.classification gave array index out of bound exception :/ (whatever it is called in python)\n",
        "def computeMetricsUsingSkLearn(args):\n",
        "    #try:\n",
        "    logits, labels = args\n",
        "    # Get predicted class labels\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    # Get predicted probabilities for the positive class\n",
        "    positive_logits = logits[:, 1]  # Assuming binary classification and 2 output classes\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n",
        "    recall = recall_score(y_true=labels, y_pred=predictions)\n",
        "    precision = precision_score(y_true=labels, y_pred=predictions)\n",
        "    f1 = f1_score(y_true=labels, y_pred=predictions)\n",
        "    roc_auc = roc_auc_score(y_true=labels, y_score=positive_logits)\n",
        "\n",
        "    return {\n",
        "      \"accuracy\": accuracy,\n",
        "      \"roc_auc\": roc_auc,\n",
        "      \"precision\": precision,\n",
        "      \"recall\": recall,\n",
        "      \"f1\": f1\n",
        "    }\n",
        "    #except Exception as x:\n",
        "    #    timber.error(f\"compute_metrics_using_sklearn failed with exception: {x}\")\n",
        "    #    return {\"accuracy\": 0, \"roc_auc\": 0, \"precision\": 0, \"recall\": 0, \"f1\": 0}\n",
        "print(\"init common completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bfef5dfd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:17:35.238463Z",
          "iopub.status.busy": "2025-04-09T04:17:35.238233Z",
          "iopub.status.idle": "2025-04-09T04:17:35.354741Z",
          "shell.execute_reply": "2025-04-09T04:17:35.353956Z"
        },
        "papermill": {
          "duration": 0.123504,
          "end_time": "2025-04-09T04:17:35.356251",
          "exception": false,
          "start_time": "2025-04-09T04:17:35.232747",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "init arguments completed\n"
          ]
        }
      ],
      "source": [
        "\"\"\" dynamic section. may be some consts,  changes based on model, etc. Try to keep it as small as possible \"\"\"\n",
        "\"\"\" THIS IS THE MOST IMPORTANT PART \"\"\"\n",
        "\n",
        "RUN_NAME = \"hyena-dna-mqtl-classifier\" # \"dna-bert-6-mqtl-classifier\" #\n",
        "MODEL_NAME = \"LongSafari/hyenadna-small-32k-seqlen-hf\" # \"zhihan1996/DNA_bert_6\" # \n",
        "SPLIT_SEQUENCE_REQUIRED=False          # False\n",
        "WINDOW = 4000  # use 200 on your local pc.\n",
        "\n",
        "SAVE_MODEL_IN_LOCAL_DIRECTORY= f\"fine-tuned-{RUN_NAME}-{WINDOW}\"\n",
        "SAVE_MODEL_IN_REMOTE_REPOSITORY = f\"fahimfarhan/{RUN_NAME}-{WINDOW}\"\n",
        "\n",
        "\n",
        "NUM_ROWS = 40_000    # hardcoded value\n",
        "PER_DEVICE_BATCH_SIZE = getDynamicBatchSize()\n",
        "EPOCHS = 10\n",
        "NUM_GPUS = max(torch.cuda.device_count(), 1)  # fallback to 1 if no GPU\n",
        "\n",
        "effective_batch_size = PER_DEVICE_BATCH_SIZE * NUM_GPUS\n",
        "STEPS_PER_EPOCH = NUM_ROWS // effective_batch_size\n",
        "MAX_STEPS = EPOCHS * STEPS_PER_EPOCH\n",
        "\n",
        "print(\"init arguments completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fdf8e72e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T04:17:35.366930Z",
          "iopub.status.busy": "2025-04-09T04:17:35.366676Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": false,
          "start_time": "2025-04-09T04:17:35.361345",
          "status": "running"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".env file loaded successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in to Hugging Face Hub successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfahimfarhan\u001b[0m (\u001b[33mnotredamians\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in to wand db successfully.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb2b7cd3e8c34636a6f01b5cfac6e11f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/981 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcdfa1253cc6450492cee9b6e9aa0da8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "configuration_hyena.py:   0%|          | 0.00/3.09k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/LongSafari/hyenadna-small-32k-seqlen-hf:\n",
            "- configuration_hyena.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model architecture: ['HyenaDNAForCausalLM']\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26ddb5e9e01649feaee717bc13342a53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2014ad08d6b4a6488acf9eac77195c3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenization_hyena.py:   0%|          | 0.00/4.06k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/LongSafari/hyenadna-small-32k-seqlen-hf:\n",
            "- tokenization_hyena.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cacfec092e7404db047179e30e5e21b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/971 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a05426bf6441473ba4790012a4713c4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_hyena.py:   0%|          | 0.00/22.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/LongSafari/hyenadna-small-32k-seqlen-hf:\n",
            "- modeling_hyena.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "769a3635eae7467eb891ac44de65eb8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of HyenaDNAForSequenceClassification were not initialized from the model checkpoint at LongSafari/hyenadna-small-32k-seqlen-hf and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb37c6ad2a8d4004b293fb35f0a77687",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "create trainer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250409_041743-pmm3u931\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhyena-dna-mqtl-classifier\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/notredamians/huggingface\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/notredamians/huggingface/runs/pmm3u931\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  19/6250 00:46 < 4:46:44, 0.36 it/s, Epoch 0.00/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\"\"\" main \"\"\"\n",
        "# def start():\n",
        "timber.info(green)\n",
        "timber.info(\"---Inside start function---\")\n",
        "timber.info(f\"{PER_DEVICE_BATCH_SIZE = }\")\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # to prevent out of memory error\n",
        "\n",
        "# wandb.init(mode=\"offline\")  # Logs only locally\n",
        "signInToHuggingFaceAndWandbToUploadModelWeightsAndBiases()\n",
        "\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "print(\"Model architecture:\", config.architectures)\n",
        "\n",
        "mainTokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "mainModel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, trust_remote_code=True, num_labels=2)\n",
        "\n",
        "isGpuAvailable = torch.cuda.is_available()\n",
        "if isGpuAvailable:\n",
        "    mainModel = mainModel.to(\"cuda\")  # not sure if it is necessary in the kaggle / huggingface virtual-machine\n",
        "\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = createPagingTrainValTestDatasets(tokenizer=mainTokenizer, window=WINDOW, splitSequenceRequired=SPLIT_SEQUENCE_REQUIRED)\n",
        "\n",
        "\n",
        "trainingArgs = TrainingArguments(\n",
        "    run_name=RUN_NAME,\n",
        "    output_dir=\"output_checkpoints\",\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_steps=STEPS_PER_EPOCH,\n",
        "    save_steps=STEPS_PER_EPOCH,\n",
        "    logging_steps=STEPS_PER_EPOCH,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n",
        "    max_steps=MAX_STEPS,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=1e-3,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_safetensors=False,\n",
        "    gradient_checkpointing=True,  # to prevent out of memory error\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "dataCollator = DataCollatorWithPadding(tokenizer=mainTokenizer)\n",
        "\n",
        "\n",
        "print(\"create trainer\")\n",
        "trainer = Trainer(\n",
        "    model=mainModel,\n",
        "    args=trainingArgs,\n",
        "    train_dataset=train_dataset,  # train\n",
        "    eval_dataset=val_dataset,  # validate\n",
        "    data_collator=dataCollator,\n",
        "    compute_metrics=computeMetricsUsingTorchEvaluate\n",
        ")\n",
        "\n",
        "\n",
        "# train, and validate\n",
        "result = trainer.train()\n",
        "try:\n",
        "    print(\"-------Training completed. Results--------\\n\")\n",
        "    print(result)\n",
        "except Exception as x:\n",
        "    print(f\"{x = }\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45105df8",
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-09T04:13:00.731Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "try:\n",
        "    print(\"-------Test completed. Results--------\\n\")\n",
        "    print(test_results)\n",
        "except Exception as x:\n",
        "    print(f\"{x = }\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f74d6a",
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-09T04:13:00.732Z"
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "mainModel.save_pretrained(save_directory=SAVE_MODEL_IN_LOCAL_DIRECTORY, safe_serialization=False)\n",
        "# push to the hub\n",
        "is_my_laptop = isMyLaptop()\n",
        "\n",
        "commit_message = f\":tada: Push model for window size {WINDOW} from huggingface space\"\n",
        "if is_my_laptop:\n",
        "  commit_message = f\":tada: Push model for window size {WINDOW} from my laptop\"\n",
        "\n",
        "mainModel.push_to_hub(\n",
        "  repo_id=SAVE_MODEL_IN_REMOTE_REPOSITORY,\n",
        "  # subfolder=f\"my-awesome-model-{WINDOW}\", subfolder didn't work :/\n",
        "  commit_message=commit_message,  # f\":tada: Push model for window size {WINDOW}\"\n",
        "  safe_serialization=False\n",
        ")\n",
        "# pass"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": null,
      "end_time": null,
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-04-09T04:16:29.082464",
      "version": "2.6.0"


    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
