{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:00:53.803151Z","iopub.execute_input":"2025-04-09T17:00:53.803439Z","iopub.status.idle":"2025-04-09T17:00:54.106435Z","shell.execute_reply.started":"2025-04-09T17:00:53.803417Z","shell.execute_reply":"2025-04-09T17:00:54.105585Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install evaluate\n!pip install huggingface\n!pip install huggingface_hub[hf_xet]  # sth to do with offline support\n!pip install python-dotenv\n!pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:01:56.470127Z","iopub.execute_input":"2025-04-09T17:01:56.470543Z","iopub.status.idle":"2025-04-09T17:02:15.774642Z","shell.execute_reply.started":"2025-04-09T17:01:56.470478Z","shell.execute_reply":"2025-04-09T17:02:15.773449Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nCollecting huggingface\n  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\nDownloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\nInstalling collected packages: huggingface\nSuccessfully installed huggingface-0.0.1\nRequirement already satisfied: huggingface_hub[hf_xet] in /usr/local/lib/python3.10/dist-packages (0.29.0)\n\u001b[33mWARNING: huggingface-hub 0.29.0 does not provide the extra 'hf-xet'\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub[hf_xet]) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[hf_xet]) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[hf_xet]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[hf_xet]) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub[hf_xet]) (2025.1.31)\nCollecting python-dotenv\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.1.0\nRequirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\"\"\" import dependencies \"\"\"\nfrom datetime import datetime\n\nimport logging\nimport os\n\nimport evaluate\nimport huggingface_hub\nfrom huggingface_hub import HfApi\nimport numpy as np\nimport wandb\nfrom datasets import load_dataset, Dataset\nfrom dotenv import load_dotenv\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom torch.utils.data import IterableDataset\nfrom transformers import BertTokenizer, BatchEncoding, AutoTokenizer, \\\n    AutoModelForSequenceClassification, AutoConfig, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # hyena dna requires this\nprint(\"import dependencies completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:02:16.727114Z","iopub.execute_input":"2025-04-09T17:02:16.727505Z","iopub.status.idle":"2025-04-09T17:02:43.891077Z","shell.execute_reply.started":"2025-04-09T17:02:16.727458Z","shell.execute_reply":"2025-04-09T17:02:43.890128Z"}},"outputs":[{"name":"stdout","text":"import dependencies completed\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\"\"\" load_env does not work in kaggle. a simple hack to reconcile the issue \"\"\"\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nos.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\nos.environ[\"WAND_DB_API_KEY\"] = user_secrets.get_secret(\"WAND_DB_API_KEY\")\n\nprint(\"Reconcile my code with kaggle\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:04:28.758758Z","iopub.execute_input":"2025-04-09T17:04:28.759166Z","iopub.status.idle":"2025-04-09T17:04:28.924609Z","shell.execute_reply.started":"2025-04-09T17:04:28.759135Z","shell.execute_reply":"2025-04-09T17:04:28.923848Z"}},"outputs":[{"name":"stdout","text":"Reconcile my code with kaggle\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"\"\"\" Common codes \"\"\"\n# some colors for visual convenience\nred = \"\\u001b[31m\"\ngreen = \"\\u001b[32m\"\nyellow = \"\\u001b[33m\"\nblue = \"\\u001b[34m\"\n\ntimber = logging.getLogger()\n# logging.basicConfig(level=logging.DEBUG)\nlogging.basicConfig(level=logging.INFO)  # change to level=logging.DEBUG to print more logs...\n\n\ndef getDynamicGpuDevice():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")  # For NVIDIA GPUs\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")  # For Apple Silicon Macs\n    else:\n        return torch.device(\"cpu\")   # Fallback to CPU\n\ndef getDynamicBatchSize():\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0).lower()\n        vramGiB = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert to GB\n\n        if \"a100\" in gpu_name:   # A100 (40GB+ VRAM)\n            batch_size = 128\n        elif \"v100\" in gpu_name:  # V100 (16GB/32GB VRAM)\n            batch_size = 64 if vramGiB >= 32 else 32\n        elif \"p100\" in gpu_name:  # P100 (16GB VRAM)\n            batch_size = 32\n        elif \"t4\" in gpu_name:    # Tesla T4 (16GB VRAM, common in Colab/Kaggle)\n            batch_size = 32  # Maybe try 64 if no OOM\n        elif \"rtx 3090\" in gpu_name or vramGiB >= 24:  # RTX 3090 (24GB VRAM)\n            batch_size = 64\n        elif vramGiB >= 16:   # Any other 16GB+ VRAM GPUs\n            batch_size = 32\n        elif vramGiB >= 8:    # 8GB VRAM GPUs (e.g., RTX 2080, 3060, etc.)\n            batch_size = 16\n        elif vramGiB >= 6:    # 6GB VRAM GPUs (e.g., RTX 2060)\n            batch_size = 8\n        else:\n            batch_size = 4  # Safe fallback for smaller GPUs\n    else:\n        batch_size = 4  # CPU mode, keep it small\n\n    return batch_size\n\ndef getGpuName():\n    gpu_name = torch.cuda.get_device_name(0).lower()\n    return gpu_name\n\n# for hyenaDna. its tokenizer can process longer sequences...\ndef sequenceEncodePlusForHyenaDna(\n    tokenizer: BertTokenizer,\n    sequence: str,\n    label: int\n) -> BatchEncoding:\n    input_ids = tokenizer(sequence)[\"input_ids\"]\n    input_ids: torch.Tensor = torch.Tensor(input_ids)\n    label_tensor = torch.tensor(label)\n    encoded_map: dict = {\n        \"input_ids\": input_ids.long(),\n        # \"attention_mask\": attention_mask.int(),    # hyenaDNA does not have attention layer\n        \"labels\": label_tensor\n    }\n\n    batchEncodingDict: BatchEncoding = BatchEncoding(encoded_map)\n    return batchEncodingDict\n\n# for dnaBert. it cannot process longer sequences...\ndef sequenceEncodePlusWithSplitting(\n        tokenizer: BertTokenizer,\n        sequence: str,\n        label: int\n) -> BatchEncoding:\n    max_size = 512\n\n    tempMap: BatchEncoding = tokenizer.encode_plus(\n        sequence,\n        add_special_tokens=False,  # we'll add the special tokens manually in the for loop below\n        return_attention_mask=True,\n        return_tensors=\"pt\"\n    )\n\n    someInputIds1xN = tempMap[\"input_ids\"]  # shape = 1xN , N = sequence length\n    someMasks1xN = tempMap[\"attention_mask\"]\n    inputIdsList = list(someInputIds1xN[0].split(510))\n    masksList = list(someMasks1xN[0].split(510))\n\n    tmpLength: int = len(inputIdsList)\n\n    for i in range(0, tmpLength):\n        cls: torch.Tensor = torch.Tensor([101])\n        sep: torch.Tensor = torch.Tensor([102])\n\n        isTokenUnitTensor = torch.Tensor([1])\n\n        inputIdsList[i]: torch.Tensor = torch.cat([\n            cls,\n            inputIdsList[i],\n            sep\n        ])\n\n        masksList[i] = torch.cat([\n            isTokenUnitTensor,\n            masksList[i],\n            isTokenUnitTensor\n        ])\n\n\n        pad_len: int = max_size - inputIdsList[i].shape[0]\n        if pad_len > 0:\n            pad: torch.Tensor = torch.Tensor([0] * pad_len)\n\n            inputIdsList[i]: torch.Tensor = torch.cat([\n                inputIdsList[i],\n                pad\n            ])\n\n            masksList[i]: torch.Tensor = torch.cat([\n                masksList[i],\n                pad\n            ])\n\n\n    # so each item len = 512, and the last one may have some padding\n    input_ids: torch.Tensor = torch.stack(inputIdsList).squeeze()  # what's with this squeeze / unsqueeze thing? o.O\n    attention_mask: torch.Tensor = torch.stack(masksList)\n    label_tensor = torch.tensor(label)\n\n    # print(f\"{input_ids.shape = }\")\n\n    encoded_map: dict = {\n        \"input_ids\": input_ids.long(),\n        \"attention_mask\": attention_mask.int(),\n        \"labels\": label_tensor\n    }\n\n    batchEncodingDict: BatchEncoding = BatchEncoding(encoded_map)\n    return batchEncodingDict\n\ndef sequenceEncodePlusCompact(\n        splitSequence: bool,\n        tokenizer: BertTokenizer,\n        sequence: str,\n        label: int\n) -> BatchEncoding:\n    if splitSequence:\n        return sequenceEncodePlusWithSplitting(tokenizer, sequence, label)\n    else:\n        return sequenceEncodePlusForHyenaDna(tokenizer, sequence, label)\n\n\nclass PagingMQTLDataset(IterableDataset):\n    def __init__(\n            self,\n            someDataset: Dataset,\n            bertTokenizer: BertTokenizer,\n            seqLength: int,\n            splitSequenceRequired: bool\n        ):\n        self.someDataset = someDataset\n        self.bertTokenizer = bertTokenizer\n        self.seqLength = seqLength\n        self.splitSequenceRequired = splitSequenceRequired\n        pass\n\n    def __iter__(self):\n        for row in self.someDataset:\n            processed = self.preprocess(row)\n            if processed is not None:\n                yield processed\n\n    def preprocess(self, row: dict):\n        sequence = row[\"sequence\"]\n        label = row[\"label\"]\n\n        if len(sequence) != self.seqLength:\n            return None  # skip a few problematic rows\n\n        return sequenceEncodePlusCompact(self.splitSequenceRequired, self.bertTokenizer, sequence, label)\n\ndef isMyLaptop() -> bool:\n    is_my_laptop = os.path.isfile(\"/home/gamegame/PycharmProjects/mqtl-classification/src/datageneration/dataset_4000_train_binned.csv\")\n    return is_my_laptop\n\n\ndef signInToHuggingFaceAndWandbToUploadModelWeightsAndBiases():\n    # Load the .env file, but don't crash if it's not found (e.g., in Hugging Face Space)\n    try:\n        load_dotenv()  # Only useful on your laptop if .env exists\n        print(\".env file loaded successfully.\")\n    except Exception as e:\n        print(f\"Warning: Could not load .env file. Exception: {e}\")\n\n    # Try to get the token from environment variables\n    try:\n        token = os.getenv(\"HF_TOKEN\")\n\n        if not token:\n            raise ValueError(\"HF_TOKEN not found. Make sure to set it in the environment variables or .env file.\")\n\n        # Log in to Hugging Face Hub\n        huggingface_hub.login(token)\n        print(\"Logged in to Hugging Face Hub successfully.\")\n\n    except Exception as e:\n        print(f\"Error during Hugging Face login: {e}\")\n        # Handle the error appropriately (e.g., exit or retry)\n\n    # wand db login\n    try:\n        api_key = os.getenv(\"WAND_DB_API_KEY\")\n        timber.info(f\"{api_key = }\")\n\n        if not api_key:\n            raise ValueError(\n                \"WAND_DB_API_KEY not found. Make sure to set it in the environment variables or .env file.\")\n\n        # Log in to Hugging Face Hub\n        wandb.login(key=api_key)\n        print(\"Logged in to wand db successfully.\")\n\n    except Exception as e:\n        print(f\"Error during wand db Face login: {e}\")\n    pass\n\ndef createPagingTrainValTestDatasets(tokenizer, window, splitSequenceRequired) -> (PagingMQTLDataset, PagingMQTLDataset, PagingMQTLDataset):\n    prefix = \"/home/gamegame/PycharmProjects/mqtl-classification/\"\n    data_files = {\n        # small samples\n        \"train_binned_200\": f\"{prefix}src/datageneration/dataset_200_train_binned.csv\",\n        \"validate_binned_200\": f\"{prefix}src/datageneration/dataset_200_validate_binned.csv\",\n        \"test_binned_200\": f\"{prefix}src/datageneration/dataset_200_test_binned.csv\",\n        # medium samples\n        \"train_binned_1000\": f\"{prefix}src/datageneration/dataset_1000_train_binned.csv\",\n        \"validate_binned_1000\": f\"{prefix}src/datageneration/dataset_1000_train_binned.csv\",\n        \"test_binned_1000\": f\"{prefix}src/datageneration/dataset_1000_train_binned.csv\",\n\n        # large samples\n        \"train_binned_4000\": f\"{prefix}src/datageneration/dataset_4000_train_binned.csv\",\n        \"validate_binned_4000\": f\"{prefix}src/datageneration/dataset_4000_train_binned.csv\",\n        \"test_binned_4000\": f\"{prefix}src/datageneration/dataset_4000_train_binned.csv\",\n    }\n\n    dataset_map = None\n    is_my_laptop = isMyLaptop()\n    if is_my_laptop:\n        dataset_map = load_dataset(\"csv\", data_files=data_files, streaming=True)\n    else:\n        dataset_map = load_dataset(\"fahimfarhan/mqtl-classification-datasets\", streaming=True)\n\n    train_dataset = PagingMQTLDataset(someDataset=dataset_map[f\"train_binned_{window}\"],\n                                    bertTokenizer=tokenizer,\n                                    seqLength=window,\n                                    splitSequenceRequired=splitSequenceRequired\n                                    )\n    val_dataset = PagingMQTLDataset(dataset_map[f\"validate_binned_{window}\"],\n                                  bertTokenizer=tokenizer,\n                                  seqLength=window,\n                                  splitSequenceRequired=splitSequenceRequired\n                                  )\n    test_dataset = PagingMQTLDataset(dataset_map[f\"test_binned_{window}\"],\n                                   bertTokenizer=tokenizer,\n                                   seqLength=window,\n                                   splitSequenceRequired=splitSequenceRequired\n                                   )\n    return train_dataset, val_dataset, test_dataset\n\n\n# Load metrics\n# global variables. bad practice...\naccuracy_metric = evaluate.load(\"accuracy\")\nf1_metric = evaluate.load(\"f1\")\nroc_auc_metric = evaluate.load(\"roc_auc\")\nprecision_metric = evaluate.load(\"precision\")\nrecall_metric = evaluate.load(\"recall\")\n\ndef computeMetricsUsingTorchEvaluate(args):\n    logits, labels = args\n    predictions = np.argmax(logits, axis=1)  # Get predicted class\n\n    positive_logits = logits[:, 1]  # Get positive class logits\n\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n    roc_auc = roc_auc_metric.compute(prediction_scores=positive_logits, references=labels)[\"roc_auc\"]  # using positive_logits repairs the error\n    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n\n    return {\n        \"accuracy\": accuracy,\n        \"roc_auc\": roc_auc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n\n# use sklearn cz torchmetrics.classification gave array index out of bound exception :/ (whatever it is called in python)\ndef computeMetricsUsingSkLearn(args):\n    #try:\n    logits, labels = args\n    # Get predicted class labels\n    predictions = np.argmax(logits, axis=1)\n\n    # Get predicted probabilities for the positive class\n    positive_logits = logits[:, 1]  # Assuming binary classification and 2 output classes\n\n    accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n    recall = recall_score(y_true=labels, y_pred=predictions)\n    precision = precision_score(y_true=labels, y_pred=predictions)\n    f1 = f1_score(y_true=labels, y_pred=predictions)\n    roc_auc = roc_auc_score(y_true=labels, y_score=positive_logits)\n\n    return {\n      \"accuracy\": accuracy,\n      \"roc_auc\": roc_auc,\n      \"precision\": precision,\n      \"recall\": recall,\n      \"f1\": f1\n    }\n    #except Exception as x:\n    #    timber.error(f\"compute_metrics_using_sklearn failed with exception: {x}\")\n    #    return {\"accuracy\": 0, \"roc_auc\": 0, \"precision\": 0, \"recall\": 0, \"f1\": 0}\nprint(\"init common completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:04:35.683009Z","iopub.execute_input":"2025-04-09T17:04:35.683357Z","iopub.status.idle":"2025-04-09T17:04:38.424735Z","shell.execute_reply.started":"2025-04-09T17:04:35.683313Z","shell.execute_reply":"2025-04-09T17:04:38.423791Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f083e97cbb424729aae28d1658610283"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ad3a4603ef4d28bb052d23d16f010c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/9.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b363cd488a7a40ecba65e25ddaf08ee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5cf05e2a4484052842432e9a654c035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a5b62f1f96c4feda3b0591184d70575"}},"metadata":{}},{"name":"stdout","text":"init common completed\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\"\"\" dynamic section. may be some consts,  changes based on model, etc. Try to keep it as small as possible \"\"\"\n\"\"\" THIS IS THE MOST IMPORTANT PART \"\"\"\n\nMODEL_NAME =  \"LongSafari/hyenadna-small-32k-seqlen-hf\"  # \"zhihan1996/DNA_bert_6\" #\nrun_name_prefix = \"hyena-dna-mqtl-classifier\"  # \"dna-bert-6-mqtl-classifier\" # \nrun_name_suffix = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\nrun_platform=\"kaggle\"\n\nRUN_NAME = f\"{run_platform}-{run_name_prefix}-{run_name_suffix}\"\nSPLIT_SEQUENCE_REQUIRED=False          # False\nWINDOW = 1000  # use 200 on your local pc.\n\nSAVE_MODEL_IN_LOCAL_DIRECTORY= f\"fine-tuned-{RUN_NAME}-{WINDOW}\"\nSAVE_MODEL_IN_REMOTE_REPOSITORY = f\"fahimfarhan/{RUN_NAME}-{WINDOW}\"\n\n\nNUM_ROWS = 1_00    # hardcoded value\nPER_DEVICE_BATCH_SIZE = getDynamicBatchSize()\nEPOCHS = 1\nNUM_GPUS = max(torch.cuda.device_count(), 1)  # fallback to 1 if no GPU\n\neffective_batch_size = PER_DEVICE_BATCH_SIZE * NUM_GPUS\nSTEPS_PER_EPOCH = NUM_ROWS // effective_batch_size\nMAX_STEPS = EPOCHS * STEPS_PER_EPOCH\n\nprint(\"init arguments completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:04:52.212650Z","iopub.execute_input":"2025-04-09T17:04:52.212973Z","iopub.status.idle":"2025-04-09T17:04:52.351873Z","shell.execute_reply.started":"2025-04-09T17:04:52.212950Z","shell.execute_reply":"2025-04-09T17:04:52.351021Z"}},"outputs":[{"name":"stdout","text":"init arguments completed\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\"\"\" main \"\"\"\ndef start():\n    timber.info(green)\n    timber.info(\"---Inside start function---\")\n    timber.info(f\"{PER_DEVICE_BATCH_SIZE = }\")\n\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n    if isMyLaptop():\n        wandb.init(mode=\"offline\")  # Logs only locally\n    else:\n        # datacenter eg huggingface or kaggle.\n        signInToHuggingFaceAndWandbToUploadModelWeightsAndBiases()\n\n    config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n    print(\"Model architecture:\", config.architectures)\n\n    mainTokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n    mainModel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, trust_remote_code=True, num_labels=2)\n\n    isGpuAvailable = torch.cuda.is_available()\n    if isGpuAvailable:\n        mainModel = mainModel.to(\"cuda\")  # not sure if it is necessary in the kaggle / huggingface virtual-machine\n\n\n    train_dataset, val_dataset, test_dataset = createPagingTrainValTestDatasets(tokenizer=mainTokenizer, window=WINDOW, splitSequenceRequired=SPLIT_SEQUENCE_REQUIRED)\n\n    trainingArgs = TrainingArguments(\n        run_name=RUN_NAME,\n        output_dir=\"output_checkpoints\",\n        eval_strategy=\"steps\",\n        save_strategy=\"steps\",\n        logging_strategy=\"steps\",\n        eval_steps=STEPS_PER_EPOCH,\n        save_steps=500,\n        logging_steps=1,  # ← more frequent logs\n        logging_first_step=True,  # ← log even the very first step\n        log_level=\"info\",  # ← control log verbosity\n        log_level_replica=\"warning\",   # ← useful if using multiple GPUs\n        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n        per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n        max_steps=MAX_STEPS,\n        weight_decay=0.01,\n        learning_rate=1e-3,\n        logging_dir=\"./logs\",\n        save_safetensors=False,\n        gradient_checkpointing=True,  # to prevent out of memory error\n        fp16=True,  # to train faster\n        report_to=\"wandb\"\n    )\n\n    dataCollator = DataCollatorWithPadding(tokenizer=mainTokenizer)\n\n\n    print(\"create trainer\")\n    trainer = Trainer(\n        model=mainModel,\n        args=trainingArgs,\n        train_dataset=train_dataset,  # train\n        eval_dataset=val_dataset,  # validate\n        data_collator=dataCollator,\n        compute_metrics=computeMetricsUsingTorchEvaluate\n    )\n\n    try:\n        # train, and validate\n        result = trainer.train()\n        print(\"-------Training completed. Results--------\\n\")\n        print(result)\n    except Exception as x:\n        print(f\"{x = }\")\n    finally:\n        # in case sth goes wrong, upload the partially trained model so that I can work with something...\n        mainModel.save_pretrained(save_directory=SAVE_MODEL_IN_LOCAL_DIRECTORY, safe_serialization=False)\n        # push to the hub\n        is_my_laptop = isMyLaptop()\n\n        commit_message = f\":tada: Push {RUN_NAME} model for window size {WINDOW} from huggingface space\"\n        if is_my_laptop:\n            commit_message = f\":tada: Push {RUN_NAME} model for window size {WINDOW} from my laptop\"\n\n        mainModel.push_to_hub(\n            repo_id=SAVE_MODEL_IN_REMOTE_REPOSITORY,\n            # subfolder=f\"my-awesome-model-{WINDOW}\", subfolder didn't work :/\n            commit_message=commit_message,  # f\":tada: Push model for window size {WINDOW}\"\n            safe_serialization=False\n        )\n\n    # todo: uncomment evaluation section\n    # test_results = trainer.evaluate(eval_dataset=test_dataset)\n    try:\n        print(\"-------Test completed. Results--------\\n\")\n        # print(test_results)\n    except Exception as x:\n        print(f\"{x = }\")\n\n\n    pass\n\nif __name__ == \"__main__\":\n    # for some reason, the variables in the main function act like global variables in python\n    # hence other functions get confused with the \"global\" variables. easiest solution, write everything\n    # in another function (say, start()), and call it inside the main\n    start()\n    pass\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:04:57.996251Z","iopub.execute_input":"2025-04-09T17:04:57.996648Z","iopub.status.idle":"2025-04-09T17:05:37.034951Z","shell.execute_reply.started":"2025-04-09T17:04:57.996615Z","shell.execute_reply":"2025-04-09T17:05:37.034011Z"}},"outputs":[{"name":"stderr","text":"Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n","output_type":"stream"},{"name":"stdout","text":".env file loaded successfully.\nLogged in to Hugging Face Hub successfully.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfahimfarhan\u001b[0m (\u001b[33mnotredamians\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Logged in to wand db successfully.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/981 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ef1119de64742bba5f0c0ad5d3f4b83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_hyena.py:   0%|          | 0.00/3.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25274e55fe1647e6a9553efcac259103"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/LongSafari/hyenadna-small-32k-seqlen-hf:\n- configuration_hyena.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"name":"stdout","text":"Model architecture: ['HyenaDNAForCausalLM']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a883fae6626b484abd97ed31efe91235"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_hyena.py:   0%|          | 0.00/4.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1438650d0e64ce2a52d441165510db2"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/LongSafari/hyenadna-small-32k-seqlen-hf:\n- tokenization_hyena.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/971 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0a17486bc374f33952e7d7738d1522f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_hyena.py:   0%|          | 0.00/22.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc52d549be744dbcafb3de2770210f7c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/LongSafari/hyenadna-small-32k-seqlen-hf:\n- modeling_hyena.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39815f1d9d7646448e8f8bcfc6923224"}},"metadata":{}},{"name":"stderr","text":"Some weights of HyenaDNAForSequenceClassification were not initialized from the model checkpoint at LongSafari/hyenadna-small-32k-seqlen-hf and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23216a2b4a564f6f87c08bdae5e0a5f5"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"name":"stdout","text":"create trainer\n","output_type":"stream"},{"name":"stderr","text":"***** Running training *****\n  Num examples = 64\n  Num Epochs = 9,223,372,036,854,775,807\n  Instantaneous batch size per device = 32\n  Training with DataParallel so batch size has been adjusted to: 64\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 1\n  Total optimization steps = 1\n  Number of trainable parameters = 3,278,080\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250409_170511-myh32zmf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/notredamians/huggingface/runs/myh32zmf' target=\"_blank\">kaggle-hyena-dna-mqtl-classifier-2025-04-09-17-04-52</a></strong> to <a href='https://wandb.ai/notredamians/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/notredamians/huggingface' target=\"_blank\">https://wandb.ai/notredamians/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/notredamians/huggingface/runs/myh32zmf' target=\"_blank\">https://wandb.ai/notredamians/huggingface/runs/myh32zmf</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 00:13, Epoch 1/9223372036854775807]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Roc Auc</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.694200</td>\n      <td>0.712737</td>\n      <td>0.499750</td>\n      <td>0.490729</td>\n      <td>0.499304</td>\n      <td>0.499750</td>\n      <td>0.404322</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples: Unknown\n  Batch size = 64\nSaving model checkpoint to output_checkpoints/checkpoint-1\nConfiguration saved in output_checkpoints/checkpoint-1/config.json\nModel weights saved in output_checkpoints/checkpoint-1/pytorch_model.bin\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nConfiguration saved in fine-tuned-kaggle-hyena-dna-mqtl-classifier-2025-04-09-17-04-52-1000/config.json\nModel weights saved in fine-tuned-kaggle-hyena-dna-mqtl-classifier-2025-04-09-17-04-52-1000/pytorch_model.bin\n","output_type":"stream"},{"name":"stdout","text":"-------Training completed. Results--------\n\nTrainOutput(global_step=1, training_loss=0.6941635608673096, metrics={'train_runtime': 22.7312, 'train_samples_per_second': 2.816, 'train_steps_per_second': 0.044, 'total_flos': 1258467065856.0, 'train_loss': 0.6941635608673096, 'epoch': 1.0})\n","output_type":"stream"},{"name":"stderr","text":"Configuration saved in /tmp/tmphb35jo_4/config.json\nModel weights saved in /tmp/tmphb35jo_4/pytorch_model.bin\nUploading the following files to fahimfarhan/kaggle-hyena-dna-mqtl-classifier-2025-04-09-17-04-52-1000: pytorch_model.bin,config.json,README.md\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b16f77104a1b43eba3c2949c14a98dd1"}},"metadata":{}},{"name":"stdout","text":"-------Test completed. Results--------\n\n","output_type":"stream"}],"execution_count":14}]}