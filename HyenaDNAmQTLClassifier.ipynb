{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-04-09T04:13:00.728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate\n!pip install huggingface\n!pip install huggingface_hub[hf_xet]  # sth to do with offline support\n!pip install python-dotenv\n!pip install wandb","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-09T04:13:00.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" import dependencies \"\"\"\nimport logging\nimport os\n\nimport evaluate\nimport huggingface_hub\nfrom huggingface_hub import HfApi\nimport numpy as np\nimport wandb\nfrom datasets import load_dataset, Dataset\nfrom dotenv import load_dotenv\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom torch.utils.data import IterableDataset\nfrom transformers import BertTokenizer, BatchEncoding, AutoTokenizer, \\\n    AutoModelForSequenceClassification, AutoConfig, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # hyena dna requires this\nprint(\"import dependencies completed\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-09T04:13:00.729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" load_env does not work in kaggle. a simple hack to reconcile the issue \"\"\"\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nos.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\nos.environ[\"WAND_DB_API_KEY\"] = user_secrets.get_secret(\"WAND_DB_API_KEY\")\n\nprint(\"Reconcile my code with kaggle\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-09T04:13:00.730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" Common codes \"\"\"\n# some colors for visual convenience\nred = \"\\u001b[31m\"\ngreen = \"\\u001b[32m\"\nyellow = \"\\u001b[33m\"\nblue = \"\\u001b[34m\"\n\ntimber = logging.getLogger()\n# logging.basicConfig(level=logging.DEBUG)\nlogging.basicConfig(level=logging.INFO)  # change to level=logging.DEBUG to print more logs...\n\n\ndef getDynamicGpuDevice():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")  # For NVIDIA GPUs\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")  # For Apple Silicon Macs\n    else:\n        return torch.device(\"cpu\")   # Fallback to CPU\n\ndef getDynamicBatchSize():\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0).lower()\n        vramGiB = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert to GB\n\n        if \"a100\" in gpu_name:   # A100 (40GB+ VRAM)\n            batch_size = 128\n        elif \"v100\" in gpu_name:  # V100 (16GB/32GB VRAM)\n            batch_size = 64 if vramGiB >= 32 else 32\n        elif \"p100\" in gpu_name:  # P100 (16GB VRAM)\n            batch_size = 32\n        elif \"t4\" in gpu_name:    # Tesla T4 (16GB VRAM, common in Colab/Kaggle)\n            batch_size = 32  # Maybe try 64 if no OOM\n        elif \"rtx 3090\" in gpu_name or vramGiB >= 24:  # RTX 3090 (24GB VRAM)\n            batch_size = 64\n        elif vramGiB >= 16:   # Any other 16GB+ VRAM GPUs\n            batch_size = 32\n        elif vramGiB >= 8:    # 8GB VRAM GPUs (e.g., RTX 2080, 3060, etc.)\n            batch_size = 16\n        elif vramGiB >= 6:    # 6GB VRAM GPUs (e.g., RTX 2060)\n            batch_size = 8\n        else:\n            batch_size = 4  # Safe fallback for smaller GPUs\n    else:\n        batch_size = 4  # CPU mode, keep it small\n\n    return batch_size\n\ndef getGpuName():\n    gpu_name = torch.cuda.get_device_name(0).lower()\n    return gpu_name\n\n# for hyenaDna. its tokenizer can process longer sequences...\ndef sequenceEncodePlusForHyenaDna(\n    tokenizer: BertTokenizer,\n    sequence: str,\n    label: int\n) -> BatchEncoding:\n    input_ids = tokenizer(sequence)[\"input_ids\"]\n    input_ids: torch.Tensor = torch.Tensor(input_ids)\n    label_tensor = torch.tensor(label)\n    encoded_map: dict = {\n        \"input_ids\": input_ids.long(),\n        # \"attention_mask\": attention_mask.int(),    # hyenaDNA does not have attention layer\n        \"labels\": label_tensor\n    }\n\n    batchEncodingDict: BatchEncoding = BatchEncoding(encoded_map)\n    return batchEncodingDict\n\n# for dnaBert. it cannot process longer sequences...\ndef sequenceEncodePlusWithSplitting(\n        tokenizer: BertTokenizer,\n        sequence: str,\n        label: int\n) -> BatchEncoding:\n    max_size = 512\n\n    tempMap: BatchEncoding = tokenizer.encode_plus(\n        sequence,\n        add_special_tokens=False,  # we'll add the special tokens manually in the for loop below\n        return_attention_mask=True,\n        return_tensors=\"pt\"\n    )\n\n    someInputIds1xN = tempMap[\"input_ids\"]  # shape = 1xN , N = sequence length\n    someMasks1xN = tempMap[\"attention_mask\"]\n    inputIdsList = list(someInputIds1xN[0].split(510))\n    masksList = list(someMasks1xN[0].split(510))\n\n    tmpLength: int = len(inputIdsList)\n\n    for i in range(0, tmpLength):\n        cls: torch.Tensor = torch.Tensor([101])\n        sep: torch.Tensor = torch.Tensor([102])\n\n        isTokenUnitTensor = torch.Tensor([1])\n\n        inputIdsList[i]: torch.Tensor = torch.cat([\n            cls,\n            inputIdsList[i],\n            sep\n        ])\n\n        masksList[i] = torch.cat([\n            isTokenUnitTensor,\n            masksList[i],\n            isTokenUnitTensor\n        ])\n\n\n        pad_len: int = max_size - inputIdsList[i].shape[0]\n        if pad_len > 0:\n            pad: torch.Tensor = torch.Tensor([0] * pad_len)\n\n            inputIdsList[i]: torch.Tensor = torch.cat([\n                inputIdsList[i],\n                pad\n            ])\n\n            masksList[i]: torch.Tensor = torch.cat([\n                masksList[i],\n                pad\n            ])\n\n\n    # so each item len = 512, and the last one may have some padding\n    input_ids: torch.Tensor = torch.stack(inputIdsList).squeeze()  # what's with this squeeze / unsqueeze thing? o.O\n    attention_mask: torch.Tensor = torch.stack(masksList)\n    label_tensor = torch.tensor(label)\n\n    # print(f\"{input_ids.shape = }\")\n\n    encoded_map: dict = {\n        \"input_ids\": input_ids.long(),\n        \"attention_mask\": attention_mask.int(),\n        \"labels\": label_tensor\n    }\n\n    batchEncodingDict: BatchEncoding = BatchEncoding(encoded_map)\n    return batchEncodingDict\n\ndef sequenceEncodePlusCompact(\n        splitSequence: bool,\n        tokenizer: BertTokenizer,\n        sequence: str,\n        label: int\n) -> BatchEncoding:\n    if splitSequence:\n        return sequenceEncodePlusWithSplitting(tokenizer, sequence, label)\n    else:\n        return sequenceEncodePlusForHyenaDna(tokenizer, sequence, label)\n\n\nclass PagingMQTLDataset(IterableDataset):\n    def __init__(\n            self,\n            someDataset: Dataset,\n            bertTokenizer: BertTokenizer,\n            seqLength: int,\n            splitSequenceRequired: bool\n        ):\n        self.someDataset = someDataset\n        self.bertTokenizer = bertTokenizer\n        self.seqLength = seqLength\n        self.splitSequenceRequired = splitSequenceRequired\n        pass\n\n    def __iter__(self):\n        for row in self.someDataset:\n            processed = self.preprocess(row)\n            if processed is not None:\n                yield processed\n\n    def preprocess(self, row: dict):\n        sequence = row[\"sequence\"]\n        label = row[\"label\"]\n\n        if len(sequence) != self.seqLength:\n            return None  # skip a few problematic rows\n\n        return sequenceEncodePlusCompact(self.splitSequenceRequired, self.bertTokenizer, sequence, label)\n\ndef isMyLaptop() -> bool:\n    is_my_laptop = os.path.isfile(\"/home/gamegame/PycharmProjects/mqtl-classification/src/datageneration/dataset_4000_train_binned.csv\")\n    return is_my_laptop\n\n\ndef signInToHuggingFaceAndWandbToUploadModelWeightsAndBiases():\n    # Load the .env file, but don't crash if it's not found (e.g., in Hugging Face Space)\n    try:\n        load_dotenv()  # Only useful on your laptop if .env exists\n        print(\".env file loaded successfully.\")\n    except Exception as e:\n        print(f\"Warning: Could not load .env file. Exception: {e}\")\n\n    # Try to get the token from environment variables\n    try:\n        token = os.getenv(\"HF_TOKEN\")\n\n        if not token:\n            raise ValueError(\"HF_TOKEN not found. Make sure to set it in the environment variables or .env file.\")\n\n        # Log in to Hugging Face Hub\n        huggingface_hub.login(token)\n        print(\"Logged in to Hugging Face Hub successfully.\")\n\n    except Exception as e:\n        print(f\"Error during Hugging Face login: {e}\")\n        # Handle the error appropriately (e.g., exit or retry)\n\n    # wand db login\n    try:\n        api_key = os.getenv(\"WAND_DB_API_KEY\")\n        timber.info(f\"{api_key = }\")\n\n        if not api_key:\n            raise ValueError(\n                \"WAND_DB_API_KEY not found. Make sure to set it in the environment variables or .env file.\")\n\n        # Log in to Hugging Face Hub\n        wandb.login(key=api_key)\n        print(\"Logged in to wand db successfully.\")\n\n    except Exception as e:\n        print(f\"Error during wand db Face login: {e}\")\n    pass\n\ndef createPagingTrainValTestDatasets(tokenizer, window, splitSequenceRequired) -> (PagingMQTLDataset, PagingMQTLDataset, PagingMQTLDataset):\n    prefix = \"/home/gamegame/PycharmProjects/mqtl-classification/\"\n    data_files = {\n        # small samples\n        \"train_binned_200\": f\"{prefix}src/datageneration/dataset_200_train_binned.csv\",\n        \"validate_binned_200\": f\"{prefix}src/datageneration/dataset_200_validate_binned.csv\",\n        \"test_binned_200\": f\"{prefix}src/datageneration/dataset_200_test_binned.csv\",\n        # medium samples\n        \"train_binned_1000\": f\"{prefix}src/datageneration/dataset_1000_train_binned.csv\",\n        \"validate_binned_1000\": f\"{prefix}src/datageneration/dataset_1000_train_binned.csv\",\n        \"test_binned_1000\": f\"{prefix}src/datageneration/dataset_1000_train_binned.csv\",\n\n        # large samples\n        \"train_binned_4000\": f\"{prefix}src/datageneration/dataset_4000_train_binned.csv\",\n        \"validate_binned_4000\": f\"{prefix}src/datageneration/dataset_4000_train_binned.csv\",\n        \"test_binned_4000\": f\"{prefix}src/datageneration/dataset_4000_train_binned.csv\",\n    }\n\n    dataset_map = None\n    is_my_laptop = isMyLaptop()\n    if is_my_laptop:\n        dataset_map = load_dataset(\"csv\", data_files=data_files, streaming=True)\n    else:\n        dataset_map = load_dataset(\"fahimfarhan/mqtl-classification-datasets\", streaming=True)\n\n    train_dataset = PagingMQTLDataset(someDataset=dataset_map[f\"train_binned_{window}\"],\n                                    bertTokenizer=tokenizer,\n                                    seqLength=window,\n                                    splitSequenceRequired=splitSequenceRequired\n                                    )\n    val_dataset = PagingMQTLDataset(dataset_map[f\"validate_binned_{window}\"],\n                                  bertTokenizer=tokenizer,\n                                  seqLength=window,\n                                  splitSequenceRequired=splitSequenceRequired\n                                  )\n    test_dataset = PagingMQTLDataset(dataset_map[f\"test_binned_{window}\"],\n                                   bertTokenizer=tokenizer,\n                                   seqLength=window,\n                                   splitSequenceRequired=splitSequenceRequired\n                                   )\n    return train_dataset, val_dataset, test_dataset\n\n\n# Load metrics\n# global variables. bad practice...\naccuracy_metric = evaluate.load(\"accuracy\")\nf1_metric = evaluate.load(\"f1\")\nroc_auc_metric = evaluate.load(\"roc_auc\")\nprecision_metric = evaluate.load(\"precision\")\nrecall_metric = evaluate.load(\"recall\")\n\ndef computeMetricsUsingTorchEvaluate(args):\n    logits, labels = args\n    predictions = np.argmax(logits, axis=1)  # Get predicted class\n\n    positive_logits = logits[:, 1]  # Get positive class logits\n\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n    roc_auc = roc_auc_metric.compute(prediction_scores=positive_logits, references=labels)[\"roc_auc\"]  # using positive_logits repairs the error\n    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n\n    return {\n        \"accuracy\": accuracy,\n        \"roc_auc\": roc_auc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1\n    }\n\n# use sklearn cz torchmetrics.classification gave array index out of bound exception :/ (whatever it is called in python)\ndef computeMetricsUsingSkLearn(args):\n    #try:\n    logits, labels = args\n    # Get predicted class labels\n    predictions = np.argmax(logits, axis=1)\n\n    # Get predicted probabilities for the positive class\n    positive_logits = logits[:, 1]  # Assuming binary classification and 2 output classes\n\n    accuracy = accuracy_score(y_true=labels, y_pred=predictions)\n    recall = recall_score(y_true=labels, y_pred=predictions)\n    precision = precision_score(y_true=labels, y_pred=predictions)\n    f1 = f1_score(y_true=labels, y_pred=predictions)\n    roc_auc = roc_auc_score(y_true=labels, y_score=positive_logits)\n\n    return {\n      \"accuracy\": accuracy,\n      \"roc_auc\": roc_auc,\n      \"precision\": precision,\n      \"recall\": recall,\n      \"f1\": f1\n    }\n    #except Exception as x:\n    #    timber.error(f\"compute_metrics_using_sklearn failed with exception: {x}\")\n    #    return {\"accuracy\": 0, \"roc_auc\": 0, \"precision\": 0, \"recall\": 0, \"f1\": 0}\nprint(\"init common completed\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-09T04:13:00.730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" dynamic section. may be some consts,  changes based on model, etc. Try to keep it as small as possible \"\"\"\n\"\"\" THIS IS THE MOST IMPORTANT PART \"\"\"\n\nRUN_NAME = \"hyena-dna-mqtl-classifier\" # \"dna-bert-6-mqtl-classifier\" #\nMODEL_NAME = \"LongSafari/hyenadna-small-32k-seqlen-hf\" # \"zhihan1996/DNA_bert_6\" # \nSPLIT_SEQUENCE_REQUIRED=False          # False\nWINDOW = 4000  # use 200 on your local pc.\n\nSAVE_MODEL_IN_LOCAL_DIRECTORY= f\"fine-tuned-{RUN_NAME}-{WINDOW}\"\nSAVE_MODEL_IN_REMOTE_REPOSITORY = f\"fahimfarhan/{RUN_NAME}-{WINDOW}\"\n\n\nNUM_ROWS = 40_000    # hardcoded value\nPER_DEVICE_BATCH_SIZE = getDynamicBatchSize()\nEPOCHS = 10\nNUM_GPUS = max(torch.cuda.device_count(), 1)  # fallback to 1 if no GPU\n\neffective_batch_size = PER_DEVICE_BATCH_SIZE * NUM_GPUS\nSTEPS_PER_EPOCH = NUM_ROWS // effective_batch_size\nMAX_STEPS = EPOCHS * STEPS_PER_EPOCH\n\nprint(\"init arguments completed\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-09T04:13:00.730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" main \"\"\"\n# def start():\ntimber.info(green)\ntimber.info(\"---Inside start function---\")\ntimber.info(f\"{PER_DEVICE_BATCH_SIZE = }\")\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # to prevent out of memory error\n\n# wandb.init(mode=\"offline\")  # Logs only locally\nsignInToHuggingFaceAndWandbToUploadModelWeightsAndBiases()\n\nconfig = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\nprint(\"Model architecture:\", config.architectures)\n\nmainTokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\nmainModel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, trust_remote_code=True, num_labels=2)\n\nisGpuAvailable = torch.cuda.is_available()\nif isGpuAvailable:\n    mainModel = mainModel.to(\"cuda\")  # not sure if it is necessary in the kaggle / huggingface virtual-machine\n\n\ntrain_dataset, val_dataset, test_dataset = createPagingTrainValTestDatasets(tokenizer=mainTokenizer, window=WINDOW, splitSequenceRequired=SPLIT_SEQUENCE_REQUIRED)\n\n\ntrainingArgs = TrainingArguments(\n    run_name=RUN_NAME,\n    output_dir=\"output_checkpoints\",\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    logging_strategy=\"steps\",\n    eval_steps=STEPS_PER_EPOCH,\n    save_steps=STEPS_PER_EPOCH,\n    logging_steps=STEPS_PER_EPOCH,\n    per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n    per_device_eval_batch_size=PER_DEVICE_BATCH_SIZE,\n    max_steps=MAX_STEPS,\n    weight_decay=0.01,\n    learning_rate=1e-3,\n    logging_dir=\"./logs\",\n    save_safetensors=False,\n    gradient_checkpointing=True,  # to prevent out of memory error\n    fp16=True\n)\n\ndataCollator = DataCollatorWithPadding(tokenizer=mainTokenizer)\n\n\nprint(\"create trainer\")\ntrainer = Trainer(\n    model=mainModel,\n    args=trainingArgs,\n    train_dataset=train_dataset,  # train\n    eval_dataset=val_dataset,  # validate\n    data_collator=dataCollator,\n    compute_metrics=computeMetricsUsingTorchEvaluate\n)\n\n\n# train, and validate\nresult = trainer.train()\ntry:\n    print(\"-------Training completed. Results--------\\n\")\n    print(result)\nexcept Exception as x:\n    print(f\"{x = }\")\n    ","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-09T04:13:00.731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_results = trainer.evaluate(eval_dataset=test_dataset)\ntry:\n    print(\"-------Test completed. Results--------\\n\")\n    print(test_results)\nexcept Exception as x:\n    print(f\"{x = }\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-09T04:13:00.731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mainModel.save_pretrained(save_directory=SAVE_MODEL_IN_LOCAL_DIRECTORY, safe_serialization=False)\n# push to the hub\nis_my_laptop = isMyLaptop()\n\ncommit_message = f\":tada: Push model for window size {WINDOW} from huggingface space\"\nif is_my_laptop:\n  commit_message = f\":tada: Push model for window size {WINDOW} from my laptop\"\n\nmainModel.push_to_hub(\n  repo_id=SAVE_MODEL_IN_REMOTE_REPOSITORY,\n  # subfolder=f\"my-awesome-model-{WINDOW}\", subfolder didn't work :/\n  commit_message=commit_message,  # f\":tada: Push model for window size {WINDOW}\"\n  safe_serialization=False\n)\n# pass","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-09T04:13:00.732Z"}},"outputs":[],"execution_count":null}]}