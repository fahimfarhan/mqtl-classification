{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-06-01T16:50:50.642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate\n!pip install huggingface\n!pip install huggingface_hub[hf_xet]  # sth to do with offline support\n!pip install python-dotenv\n!pip install wandb","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T16:50:50.643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nimport os\n\nimport evaluate\nimport huggingface_hub\nfrom huggingface_hub import HfApi\nimport numpy as np\nimport wandb\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom dotenv import load_dotenv\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nfrom torch.utils.data import IterableDataset, get_worker_info\nfrom transformers import BertTokenizer, BatchEncoding, AutoTokenizer, \\\n    AutoModelForSequenceClassification, AutoConfig, TrainingArguments, Trainer, DataCollatorWithPadding\nimport torch\nimport warnings\nimport torch\n\nimport wandb\n\ntimber = logging.getLogger()\n# logging.basicConfig(level=logging.DEBUG)\nlogging.basicConfig(level=logging.INFO)  # change to level=logging.DEBUG to print more logs...\n\n# some colors for visual convenience\nred = \"\\u001b[31m\"\ngreen = \"\\u001b[32m\"\nyellow = \"\\u001b[33m\"\nblue = \"\\u001b[34m\"\n\ndef getDynamicGpuDevice():\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")  # For NVIDIA GPUs\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")  # For Apple Silicon Macs\n    else:\n        return torch.device(\"cpu\")   # Fallback to CPU\n\ndef getDynamicBatchSize():\n    if torch.cuda.is_available():\n        gpu_name = torch.cuda.get_device_name(0).lower()\n        vramGiB = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert to GB\n\n        if \"a100\" in gpu_name:   # A100 (40GB+ VRAM)\n            batch_size = 128\n        elif \"v100\" in gpu_name:  # V100 (16GB/32GB VRAM)\n            batch_size = 64 if vramGiB >= 32 else 32\n        elif \"p100\" in gpu_name:  # P100 (16GB VRAM)\n            batch_size = 32\n        elif \"t4\" in gpu_name:    # Tesla T4 (16GB VRAM, common in Colab/Kaggle)\n            batch_size = 32  # Maybe try 64 if no OOM\n        elif \"rtx 3090\" in gpu_name or vramGiB >= 24:  # RTX 3090 (24GB VRAM)\n            batch_size = 64\n        elif vramGiB >= 16:   # Any other 16GB+ VRAM GPUs\n            batch_size = 32\n        elif vramGiB >= 8:    # 8GB VRAM GPUs (e.g., RTX 2080, 3060, etc.)\n            batch_size = 16\n        elif vramGiB >= 6:    # 6GB VRAM GPUs (e.g., RTX 2060)\n            batch_size = 8\n        else:\n            batch_size = 4  # Safe fallback for smaller GPUs\n    else:\n        batch_size = 4  # CPU mode, keep it small\n\n    return batch_size\n\ndef getGpuName():\n    gpu_name = torch.cuda.get_device_name(0).lower()\n    return gpu_name\n\n\ndef toKmerSequence(seq: str, k: int=6) -> str:\n    \"\"\"\n    :param seq:  ATCGTTCAATCGTTCA.........\n    :param k: 6\n    :return: ATCGTT CAATCG TTCA.. ...... ......\n    \"\"\"\n\n    output = \"\"\n    for i in range(len(seq) - k + 1):\n        output = output + seq[i:i + k] + \" \"\n    return output\n\n\ndef pretty_print_metrics(metrics: dict, stage: str = \"\"):\n    metrics_str = f\"\\nðŸ“Š {stage} Metrics:\\n\" + \"\\n\".join(\n        f\"  {k:>15}: {v:.4f}\" if v is not None else f\"  {k:>15}: N/A\"\n        for k, v in metrics.items()\n    )\n    print(metrics_str)\n\nclass PagingMQTLDataset(IterableDataset):\n    def __init__(\n            self,\n            someDataset: Dataset,\n            bertTokenizer: BertTokenizer,\n            seqLength: int,\n            toKmer: bool,\n            datasetLen: int\n        ):\n        self.someDataset = someDataset\n        self.bertTokenizer = bertTokenizer\n        self.seqLength = seqLength\n        self.toKmer = toKmer\n        self.datasetLen=datasetLen\n        pass\n\n    \"\"\"\n    # if you're using lightning ai, don't define the __len__. \n    # But in Huggingface transformer, setting the __len__ was kinda convenient\n    \n    def __len__(self):\n        return self.datasetLen\n    \"\"\"\n\n    def createShardDatasetForMultipleGpus(self):\n        worker_info = get_worker_info()\n\n        rank = int(os.environ.get(\"RANK\", 0))\n        world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\n        # Total shards = world_size * num_workers (or 1 if no workers)\n        num_workers = worker_info.num_workers if worker_info is not None else 1\n        worker_id = worker_info.id if worker_info is not None else 0\n\n        timber.info(f\"{num_workers = }, {worker_id = }, {rank = }, {world_size=}\")\n\n        total_shards = world_size * num_workers\n        shard_index = rank * num_workers + worker_id\n        # Shard the dataset accordingly\n        shard_dataset = self.someDataset.shard(num_shards=total_shards, index=shard_index)\n        return shard_dataset\n\n    def __iter__(self):\n        shardDataset = self.createShardDatasetForMultipleGpus()\n\n        for row in shardDataset:\n            processed = self.preprocess(row)\n            if processed is not None:\n                yield processed\n\n    def preprocess(self, row: dict):\n        raise Exception(\"Please override this function\")\n\ndef isMyLaptop() -> bool:\n    is_my_laptop = os.path.isdir(\"/home/gamegame/\")\n    return is_my_laptop\n\ndef signInToHuggingFaceAndWandbToUploadModelWeightsAndBiases():\n    # Load the .env file, but don't crash if it's not found (e.g., in Hugging Face Space)\n    try:\n        load_dotenv()  # Only useful on your laptop if .env exists\n        print(\".env file loaded successfully.\")\n    except Exception as e:\n        print(f\"Warning: Could not load .env file. Exception: {e}\")\n\n    # Try to get the token from environment variables\n    try:\n        token = os.getenv(\"HF_TOKEN\")\n\n        if not token:\n            raise ValueError(\"HF_TOKEN not found. Make sure to set it in the environment variables or .env file.\")\n\n        # Log in to Hugging Face Hub\n        huggingface_hub.login(token)\n        print(\"Logged in to Hugging Face Hub successfully.\")\n\n    except Exception as e:\n        print(f\"Error during Hugging Face login: {e}\")\n        # Handle the error appropriately (e.g., exit or retry)\n\n    # wand db login\n    try:\n        api_key = os.getenv(\"WAND_DB_API_KEY\")\n        timber.info(f\"{api_key = }\")\n\n        if not api_key:\n            raise ValueError(\n                \"WAND_DB_API_KEY not found. Make sure to set it in the environment variables or .env file.\")\n\n        # Log in to Hugging Face Hub\n        wandb.login(key=api_key)\n        print(\"Logged in to wand db successfully.\")\n\n    except Exception as e:\n        print(f\"Error during wand db Face login: {e}\")\n    pass\n\ndef get_dataset_length(dataset_name=None, split=None, local_path=None):\n    try:\n        if local_path:\n            dataset = load_dataset(\"csv\", data_files={split: local_path}, split=split, streaming=False)\n        else:\n            dataset = load_dataset(dataset_name, split=split, streaming=False)\n        return len(dataset)\n    except Exception as e:\n        print(f\"Error while loading length for {split}: {e}\")\n        return None\n\ndef createSinglePagingDatasets(\n        data_files,\n        split,\n        tokenizer,\n        window,\n        splitSequenceRequired\n) -> PagingMQTLDataset:  # I can't come up with creative names\n    is_my_laptop = isMyLaptop()\n    if is_my_laptop:\n        dataset_map = load_dataset(\"csv\", data_files=data_files, streaming=True)\n        dataset_len = get_dataset_length(local_path=data_files[split], split=split)\n    else:\n        dataset_map = load_dataset(\"fahimfarhan/mqtl-classification-datasets\", streaming=True)\n        dataset_len = get_dataset_length(dataset_name=\"fahimfarhan/mqtl-classification-datasets\", split=split)\n\n    print(f\"{split = } ==> {dataset_len = }\")\n    return PagingMQTLDataset(\n        someDataset=dataset_map[f\"train_binned_{window}\"],\n        bertTokenizer=tokenizer,\n        seqLength=window,\n        toKmer=splitSequenceRequired,\n        datasetLen = dataset_len\n    )\n\n\ndef createPagingTrainValTestDatasets(tokenizer, window, toKmer) -> (PagingMQTLDataset, PagingMQTLDataset, PagingMQTLDataset):\n    prefix = \"/home/gamegame/PycharmProjects/mqtl-classification/src/datageneration/\"\n\n    data_files = {\n        # small\n        \"train_binned_1029\": f\"{prefix}dataset_1029_train_binned.csv\",\n        \"validate_binned_1029\": f\"{prefix}dataset_1029_validate_binned.csv\",\n        \"test_binned_1029\": f\"{prefix}dataset_1029_train_binned.csv\",\n\n        # medium\n        \"train_binned_2053\": f\"{prefix}dataset_1029_train_binned.csv\",\n        \"validate_binned_2053\": f\"{prefix}dataset_1029_validate_binned.csv\",\n        \"test_binned_2053\": f\"{prefix}dataset_1029_test_binned.csv\",\n\n        # large\n        \"train_binned_4101\": f\"{prefix}dataset_1029_train_binned.csv\",\n        \"validate_binned_4101\": f\"{prefix}dataset_1029_validate_binned.csv\",\n        \"test_binned_4101\": f\"{prefix}dataset_1029_test_binned.csv\",\n    }\n\n    # not sure if this is a good idea. if anything goes wrong, revert back to previous code of this function\n    train_dataset = createSinglePagingDatasets(data_files, f\"train_binned_{window}\", tokenizer, window, toKmer)\n\n    val_dataset =createSinglePagingDatasets(data_files, f\"validate_binned_{window}\", tokenizer, window, toKmer)\n\n    test_dataset = createSinglePagingDatasets(data_files, f\"test_binned_{window}\", tokenizer, window, toKmer)\n\n    return train_dataset, val_dataset, test_dataset\n\ndef disableAnnoyingWarnings():\n    # Caution! if anything goes wrong, enable it. make sure this warning related issue ain't the culprit!\n    warnings.filterwarnings(\n        \"ignore\",\n        message=\"Length of IterableDataset\",\n        category=UserWarning,\n        module=\"torch.utils.data.dataloader\"\n    )\n\nclass ComputeMetricsUsingSkLearn:\n    def __init__(self):\n        self.logits = []\n        self.labels = []\n\n    def update(self, logits: torch.Tensor, labels: torch.Tensor):\n        self.logits.append(logits.detach().cpu())\n        self.labels.append(labels.detach().cpu())\n\n    def compute(self):\n        if not self.logits or not self.labels:\n            return {}\n\n        logits = torch.cat(self.logits).numpy()\n        labels = torch.cat(self.labels).numpy()\n        predictions = np.argmax(logits, axis=1)\n\n        try:\n            positive_logits = logits[:, 1] if logits.shape[1] > 1 else logits[:, 0]\n        except IndexError as e:\n            print(\"Logit indexing failed:\", e)\n            return {}\n\n        try:\n            return {\n                \"accuracy\": accuracy_score(labels, predictions),\n                \"roc_auc\": roc_auc_score(labels, positive_logits),\n                \"precision\": precision_score(labels, predictions, average=\"weighted\", zero_division=0),\n                \"recall\": recall_score(labels, predictions, average=\"weighted\", zero_division=0),\n                \"f1\": f1_score(labels, predictions, average=\"weighted\", zero_division=0)\n            }\n        except Exception as e:\n            print(f\">> Metrics computation failed: {e}\")\n            return {}\n\n    def clear(self):\n        self.logits.clear()\n        self.labels.clear()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T16:50:50.644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nthe steps:\n*. load model, tokenizer,\n*. create Datasets object,\n*. init trainer_args object\n*. create custom metrics function\n*. other util functions (dynamic gpu, dynamic batch size, etc)\n*. init, and run trainer object,\n*. run on eval dataset\n* push model to huggingface\n* push weights, & biases to wandb\n* save the kaggle notebook result into github\n\"\"\"\n\n\"\"\" import dependencies \"\"\"\nfrom datetime import datetime\nfrom typing import Optional, Union\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.types import STEP_OUTPUT\nfrom torch import nn\nfrom torch.nn import MSELoss, CrossEntropyLoss\nfrom torch.optim import AdamW, Optimizer\nfrom torch.utils.data import DataLoader\nfrom transformers import BertPreTrainedModel, AutoModel, AutoConfig, AutoTokenizer\nfrom transformers.models.bert.modeling_bert import BERT_START_DOCSTRING, BertModel, BERT_INPUTS_DOCSTRING","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T16:50:50.644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" dynamic section. may be some consts,  changes based on model, etc. Try to keep it as small as possible \"\"\"\n\"\"\" THIS IS THE MOST IMPORTANT PART \"\"\"\n\n# MODEL_NAME = \"LongSafari/hyenadna-small-32k-seqlen-hf\"\n# run_name_prefix = \"hyena-dna-mqtl-classifier\"\nMODEL_NAME =  \"zhihan1996/DNA_bert_6\"\nrun_name_prefix = \"dna-bert-6-mqtl-classifier\"\n\nrun_name_suffix = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\nrun_platform=\"kaggle\"\n\nRUN_NAME = f\"{run_platform}-{run_name_prefix}-{run_name_suffix}\"\nCONVERT_TO_KMER= (MODEL_NAME == \"zhihan1996/DNA_bert_6\")\nWINDOW = 1024  # use 200 on your local pc.\n\nSAVE_MODEL_IN_LOCAL_DIRECTORY= f\"fine-tuned-{RUN_NAME}-{WINDOW}\"\nSAVE_MODEL_IN_REMOTE_REPOSITORY = f\"fahimfarhan/{RUN_NAME}-{WINDOW}\"\n\n\nNUM_ROWS = 2_000    # hardcoded value\nPER_DEVICE_BATCH_SIZE = getDynamicBatchSize()\nEPOCHS = 1\nNUM_GPUS = max(torch.cuda.device_count(), 1)  # fallback to 1 if no GPU\n\neffective_batch_size = PER_DEVICE_BATCH_SIZE * NUM_GPUS\nSTEPS_PER_EPOCH = NUM_ROWS // effective_batch_size\nMAX_STEPS = EPOCHS * STEPS_PER_EPOCH\n\nprint(\"init arguments completed\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T16:50:50.645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # hyena dna requires this\nprint(\"import dependencies completed\")\n\n\"\"\" Common codes \"\"\"\nclass DNaBert6PagingMQTLDataset(PagingMQTLDataset):\n    def preprocess(self, row: dict):\n        sequence = row[\"sequence\"]\n        label = row[\"label\"]\n\n        kmerSeq = toKmerSequence(sequence)\n        kmerSeqTokenized = self.bertTokenizer(\n            kmerSeq,\n            max_length=self.seqLength, # 2048,\n            padding='max_length',\n            return_tensors=\"pt\"\n        )\n        input_ids = kmerSeqTokenized[\"input_ids\"]\n        attention_mask = kmerSeqTokenized[\"attention_mask\"]\n        input_ids: torch.Tensor = torch.Tensor(input_ids)\n        attention_mask = torch.Tensor(attention_mask)\n        label_tensor = torch.tensor(label)\n        encoded_map: dict = {\n            \"input_ids\": input_ids.long(),\n            \"attention_mask\": attention_mask.int(),  # hyenaDNA does not have attention layer\n            \"labels\": label_tensor\n        }\n        return encoded_map\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T16:50:50.645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\" main \"\"\"\n\n\ndef add_start_docstrings(*docstr):\n    def docstring_decorator(fn):\n        fn.__doc__ = \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n        return fn\n\n    return docstring_decorator\n\ndef add_start_docstrings_to_callable(*docstr):\n    def docstring_decorator(fn):\n        class_name = \":class:`~transformers.{}`\".format(fn.__qualname__.split(\".\")[0])\n        intro = \"   The {} forward method, overrides the :func:`__call__` special method.\".format(class_name)\n        note = r\"\"\"\n\n    .. note::\n        Although the recipe for forward pass needs to be defined within\n        this function, one should call the :class:`Module` instance afterwards\n        instead of this since the former takes care of running the\n        pre and post processing steps while the latter silently ignores them.\n        \"\"\"\n        fn.__doc__ = intro + note + \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n        return fn\n\n    return docstring_decorator\n\n\n@add_start_docstrings(\n    \"\"\"Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\n    the pooled output) e.g. for GLUE tasks. Especially designed for sequences longer than 512. \"\"\",\n    BERT_START_DOCSTRING,\n)\nclass BertForLongSequenceClassification(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.split = config.split\n        self.rnn_type = config.rnn\n        self.num_rnn_layer = config.num_rnn_layer\n        self.hidden_size = config.hidden_size\n        self.rnn_dropout = config.rnn_dropout\n        self.rnn_hidden = config.rnn_hidden\n\n        self.bert = BertModel(config)\n        if self.rnn_type == \"lstm\":\n            self.rnn = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, bidirectional=True,\n                               num_layers=self.num_rnn_layer, batch_first=True, dropout=self.rnn_dropout)\n        elif self.rnn_type == \"gru\":\n            self.rnn = nn.GRU(input_size=self.hidden_size, hidden_size=self.hidden_size, bidirectional=True,\n                              num_layers=self.num_rnn_layer, batch_first=True, dropout=self.rnn_dropout)\n        else:\n            raise ValueError\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(self.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    @add_start_docstrings_to_callable(BERT_INPUTS_DOCSTRING)\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            labels=None,\n            overlap=100,\n            max_length_per_seq=500,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n\n    Returns:\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n            Classification (or regression if config.num_labels==1) loss.\n        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n\n    Examples::\n\n        from transformers import BertTokenizer, BertForSequenceClassification\n        import torch\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n\n        loss, logits = outputs[:2]\n\n        \"\"\"\n        # batch_size = input_ids.shape[0]\n        # sequence_length = input_ids.shape[1]\n        # starts = []\n        # start = 0\n        # while start + max_length_per_seq <= sequence_length:\n        #     starts.append(start)\n        #     start += (max_length_per_seq-overlap)\n        # last_start = sequence_length-max_length_per_seq\n        # if last_start > starts[-1]:\n        #     starts.append(last_start)\n\n        # new_input_ids = torch.zeros([len(starts)*batch_size, max_length_per_seq], dtype=input_ids.dtype, device=input_ids.device)\n        # new_attention_mask = torch.zeros([len(starts)*batch_size, max_length_per_seq], dtype=attention_mask.dtype, device=attention_mask.device)\n        # new_token_type_ids = torch.zeros([len(starts)*batch_size, max_length_per_seq], dtype=token_type_ids.dtype, device=token_type_ids.device)\n\n        # for j in range(batch_size):\n        #     for i, start in enumerate(starts):\n        #         new_input_ids[i] = input_ids[j,start:start+max_length_per_seq]\n        #         new_attention_mask[i] = attention_mask[j,start:start+max_length_per_seq]\n        #         new_token_type_ids[i] = token_type_ids[j,start:start+max_length_per_seq]\n\n        # if batch_size == 1:\n        #     pooled_output = outputs[1].mean(dim=0)\n        #     pooled_output = pooled_output.reshape(1, pooled_output.shape[0])\n        # else:\n        #     pooled_output = torch.zeros([batch_size, outputs[1].shape[1]], dtype=outputs[1].dtype)\n        #     for i in range(batch_size):\n        #         pooled_output[i] = outputs[1][i*batch_size:(i+1)*batch_size].mean(dim=0)\n\n        batch_size = input_ids.shape[0]\n        # print(f\"debug: {batch_size = }\")\n        # print(f\"debug: {self.split = }\")\n        # print(f\"debug: {input_ids.shape = }\")\n        # print(f\"debug: {input_ids.size = }\")\n        input_ids = input_ids.view(self.split * batch_size, 512)\n        attention_mask = attention_mask.view(self.split * batch_size, 512)\n        token_type_ids = None\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        # lstm\n        if self.rnn_type == \"lstm\":\n            # random\n            # h0 = autograd.Variable(torch.randn([2*self.num_rnn_layer, batch_size, self.hidden_size], device=input_ids.device))/100.0\n            # c0 = autograd.Variable(torch.randn([2*self.num_rnn_layer, batch_size, self.hidden_size], device=input_ids.device))/100.0\n            # self.hidden = (h0, c0)\n            # self.rnn.flatten_parameters()\n            # pooled_output = outputs[1].view(batch_size, self.split, self.hidden_size)\n            # _, (ht, ct) = self.rnn(pooled_output, self.hidden)\n\n            # orth\n            # h0 = torch.empty([2*self.num_rnn_layer, batch_size, self.hidden_size], device=input_ids.device)\n            # nn.init.orthogonal_(h0)\n            # h0 = autograd.Variable(h0)\n            # c0 = torch.empty([2*self.num_rnn_layer, batch_size, self.hidden_size], device=input_ids.device)\n            # nn.init.orthogonal_(c0)\n            # c0 = autograd.Variable(c0)\n            # self.hidden = (h0, c0)\n            # pooled_output = outputs[1].view(batch_size, self.split, self.hidden_size)\n            # _, (ht, ct) = self.rnn(pooled_output, self.hidden)\n\n            # zero\n            pooled_output = outputs[1].view(batch_size, self.split, self.hidden_size)\n            _, (ht, ct) = self.rnn(pooled_output)\n        elif self.rnn_type == \"gru\":\n            # h0 = autograd.Variable(torch.randn([2*self.num_rnn_layer, batch_size, self.hidden_size], device=input_ids.device))\n            # pooled_output = outputs[1].view(batch_size, self.split, self.hidden_size)\n            # _, ht = self.rnn(pooled_output, h0)\n\n            # h0 = torch.empty([2*self.num_rnn_layer, batch_size, self.hidden_size], device=input_ids.device)\n            # nn.init.orthogonal_(h0)\n            # h0 = autograd.Variable(h0)\n            # pooled_output = outputs[1].view(batch_size, self.split, self.hidden_size)\n            # _, ht = self.rnn(pooled_output, h0)\n\n            pooled_output = outputs[1].view(batch_size, self.split, self.hidden_size)\n            _, ht = self.rnn(pooled_output)\n        else:\n            raise ValueError\n\n        output = self.dropout(ht.squeeze(0).sum(dim=0))\n        logits = self.classifier(output)\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\n\n\nclass MQTLClassifierModule(pl.LightningModule):\n    def __init__(self, model, learning_rate=5e-5, weight_decay=0.0, max_grad_norm=1.0):\n        super().__init__()\n        self.model = model\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.max_grad_norm = max_grad_norm\n        self.criterion = torch.nn.CrossEntropyLoss()\n\n        self.train_metrics = ComputeMetricsUsingSkLearn()\n        self.val_metrics = ComputeMetricsUsingSkLearn()\n        self.test_metrics = ComputeMetricsUsingSkLearn()\n\n    def forward(self, batch):\n        loss, logits = self.model(**batch)\n        return loss, logits\n\n    def training_step(self, batch, batch_idx)-> STEP_OUTPUT:\n        with torch.autograd.set_detect_anomaly(True):  # Anomaly detection enabled here\n            labels = batch[\"labels\"]\n            loss, logits = self.forward(batch)\n\n            # Log the loss\n            self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n\n            # Update training metrics\n            self.train_metrics.update(logits=logits, labels=labels)\n\n            return loss\n\n    def on_after_backward(self):\n        # Compute and log gradient norm\n        total_norm = 0.0\n        for name, param in self.named_parameters():\n            if param.grad is not None:\n                # self.logger.experiment.add_histogram(f\"{name}_grad\", param.grad, self.global_step)\n                # self.logger.experiment.add_histogram(f\"{name}_weights\", param, self.global_step)\n                param_norm = param.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n        total_norm = total_norm ** 0.5\n        self.log(\"grad_norm\", total_norm, prog_bar=True, on_step=True, on_epoch=False)\n\n    def on_train_epoch_end(self) -> None:\n        metrics = self.train_metrics.compute()\n        self.train_metrics.clear()\n        # for k, v in metrics.items():\n        #     self.log(f\"train_{k}\", v, prog_bar=True)\n        pretty_print_metrics(metrics, \"Train\")\n        pass\n\n    def validation_step(self, batch, batch_idx) -> STEP_OUTPUT:\n        labels = batch[\"labels\"]\n        loss, logits = self.forward(batch)\n        self.val_metrics.update(logits=logits, labels=labels)\n        return loss\n\n    def on_validation_epoch_end(self) -> None:\n        metrics = self.val_metrics.compute()\n        self.val_metrics.clear()\n        # for k, v in metrics.items():\n        #     self.log(f\"eval_{k}\", v, prog_bar=True)\n        pretty_print_metrics(metrics, \"Eval\")\n        pass\n\n    def test_step(self, batch, batch_idx) -> STEP_OUTPUT:\n        labels = batch[\"labels\"]\n        loss, logits = self.forward(batch)\n        self.test_metrics.update(logits=logits, labels=labels)\n        return loss\n\n    def on_test_epoch_end(self) -> None:\n        metrics = self.test_metrics.compute()\n        self.test_metrics.clear()\n        # for k, v in metrics.items():\n        #     self.log(f\"test_{k}\", v, prog_bar=True)\n        pretty_print_metrics(metrics, \"Test\")\n        pass\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        return optimizer\n\n    def configure_gradient_clipping(\n        self,\n        optimizer: Optimizer,\n        gradient_clip_val: Optional[Union[int, float]] = None,\n        gradient_clip_algorithm: Optional[str] = None,\n    ) -> None:\n        if self.max_grad_norm is not None:\n            torch.nn.utils.clip_grad_norm_(self.parameters(), self.max_grad_norm)\n\nclass DnaBert6PagingMQTLDataset(PagingMQTLDataset):\n    def preprocess(self, row: dict):\n        sequence = row[\"sequence\"]\n        label = row[\"label\"]\n\n        kmerSeq = toKmerSequence(sequence)\n        kmerSeqTokenized = self.bertTokenizer(\n            kmerSeq,\n            max_length=WINDOW,\n            padding='max_length',\n            return_tensors=\"pt\"\n        )\n        input_ids = kmerSeqTokenized[\"input_ids\"]\n        attention_mask = kmerSeqTokenized[\"attention_mask\"]\n        input_ids: torch.Tensor = torch.Tensor(input_ids)\n        attention_mask = torch.Tensor(attention_mask)\n        label_tensor = torch.tensor(label)\n        encoded_map: dict = {\n            \"input_ids\": input_ids.long(),\n            \"attention_mask\": attention_mask.int(),  # hyenaDNA does not have attention layer\n            \"labels\": label_tensor\n        }\n\n        return encoded_map\n\n\ndef createSingleDnaBert6PagingDatasets(\n        data_files,\n        split,\n        tokenizer,\n        window,\n        splitSequenceRequired\n) -> DnaBert6PagingMQTLDataset:  # I can't come up with creative names\n    is_my_laptop = isMyLaptop()\n    if is_my_laptop:\n        dataset_map = load_dataset(\"csv\", data_files=data_files, streaming=True)\n        dataset_len = get_dataset_length(local_path=data_files[split], split=split)\n    else:\n        dataset_map = load_dataset(\"fahimfarhan/mqtl-classification-datasets\", streaming=True)\n        dataset_len = get_dataset_length(dataset_name=\"fahimfarhan/mqtl-classification-datasets\", split=split)\n\n    print(f\"{split = } ==> {dataset_len = }\")\n    return DnaBert6PagingMQTLDataset(\n        someDataset=dataset_map[f\"train_binned_{window}\"],\n        bertTokenizer=tokenizer,\n        seqLength=window,\n        toKmer=splitSequenceRequired,\n        datasetLen = dataset_len\n    )\n\ndef createDnaBert6PagingTrainValTestDatasets(tokenizer, window, toKmer) -> (DnaBert6PagingMQTLDataset, DnaBert6PagingMQTLDataset, DnaBert6PagingMQTLDataset):\n    prefix = \"/home/gamegame/PycharmProjects/mqtl-classification/src/datageneration/\"\n\n    data_files = {\n        # small\n        \"train_binned_1027\": f\"{prefix}_1027_train_binned.csv\",\n        \"validate_binned_1027\": f\"{prefix}_1027_validate_binned.csv\",\n        \"test_binned_1027\": f\"{prefix}_1027_train_binned.csv\",\n\n        # medium\n        \"train_binned_2051\": f\"{prefix}_2051_train_binned.csv\",\n        \"validate_binned_2051\": f\"{prefix}_2051_validate_binned.csv\",\n        \"test_binned_2051\": f\"{prefix}_2051_test_binned.csv\",\n\n        # large\n        \"train_binned_4099\": f\"{prefix}_4099_train_binned.csv\",\n        \"validate_binned_4099\": f\"{prefix}_4099_validate_binned.csv\",\n        \"test_binned_4099\": f\"{prefix}_4099_test_binned.csv\",\n    }\n\n    # not sure if this is a good idea. if anything goes wrong, revert back to previous code of this function\n    train_dataset = createSingleDnaBert6PagingDatasets(data_files, f\"train_binned_{window}\", tokenizer, window, toKmer)\n\n    val_dataset =createSingleDnaBert6PagingDatasets(data_files, f\"validate_binned_{window}\", tokenizer, window, toKmer)\n\n    test_dataset = createSingleDnaBert6PagingDatasets(data_files, f\"test_binned_{window}\", tokenizer, window, toKmer)\n\n    return train_dataset, val_dataset, test_dataset\n\n\ndef save_fine_tuned_model(mainModel):\n    # save the model in huggingface repository, and local storage\n    mainModel.save_pretrained(save_directory=SAVE_MODEL_IN_LOCAL_DIRECTORY, safe_serialization=False)\n    # push to the hub\n    is_my_laptop = isMyLaptop()\n\n    if is_my_laptop:  # no need to save\n        return\n\n    # commit_message = f\":tada: Push {RUN_NAME} model for window size {WINDOW} from my laptop\"\n    commit_message = f\":tada: Push {RUN_NAME} model for window size {WINDOW} into huggingface hub\"\n    mainModel.push_to_hub(\n        repo_id=SAVE_MODEL_IN_REMOTE_REPOSITORY,\n        # subfolder=f\"my-awesome-model-{WINDOW}\", subfolder didn't work :/\n        commit_message=commit_message,\n        safe_serialization=False\n    )\n\ndef start():\n    timber.info(green)\n    timber.info(\"---Inside start function---\")\n    timber.info(f\"{PER_DEVICE_BATCH_SIZE = }\")\n\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n    disableAnnoyingWarnings()\n\n    if isMyLaptop():\n        wandb.init(mode=\"offline\")  # Logs only locally\n    else:\n        # datacenter eg huggingface or kaggle.\n        signInToHuggingFaceAndWandbToUploadModelWeightsAndBiases()\n\n\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # OutOfMemoryError\n\n    model_name = MODEL_NAME\n\n    dnaTokenizer = BertTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    baseModel = AutoModel.from_pretrained(model_name, trust_remote_code=True) # this is the correct way to load pretrained weights, and modify config\n    baseModel.gradient_checkpointing_enable()  #  bert model's builtin way to enable gradient check pointing\n\n    print(\"-------update some more model configs start-------\")\n    baseModel.resize_token_embeddings(len(dnaTokenizer))\n    baseModel.config.max_position_embeddings = WINDOW\n    baseModel.embeddings.position_embeddings = torch.nn.Embedding(WINDOW, baseModel.config.hidden_size)\n    print(baseModel)\n    print(\"--------update some more model configs end--------\")\n\n    someConfig = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n    someConfig.split = (WINDOW // 512)  # hmm. so it works upto 7 on my laptop. if 8, then OutOfMemoryError\n    # mainModel = BertForLongSequenceClassification.from_pretrained(model_name, config=someConfig, trust_remote_code=True) # this is the correct way to load pretrained weights, and modify config\n    someConfig.max_position_embeddings = WINDOW\n    someConfig.rnn = \"gru\" # or \"lstm\". Let's check if it works\n    mainModel = BertForLongSequenceClassification(someConfig)\n    mainModel.bert = baseModel\n\n    print(mainModel)\n\n    dataCollator = DataCollatorWithPadding(tokenizer=dnaTokenizer)\n\n    # L = T + k - 3 [for dna bert 6, we have 2 extra tokens, cls, and sep]\n    rawSequenceLength = WINDOW + 6 - 3\n    train_dataset, val_dataset, test_dataset = createDnaBert6PagingTrainValTestDatasets(tokenizer=dnaTokenizer, window=rawSequenceLength, toKmer=CONVERT_TO_KMER)\n\n\n    train_loader = DataLoader(train_dataset, batch_size=PER_DEVICE_BATCH_SIZE, shuffle=False, collate_fn=dataCollator) # Can't shuffle the paging/streaming datasets\n    val_loader = DataLoader(val_dataset, batch_size=PER_DEVICE_BATCH_SIZE, shuffle=False, collate_fn=dataCollator)\n    test_loader = DataLoader(test_dataset, batch_size=PER_DEVICE_BATCH_SIZE, shuffle=False, collate_fn=dataCollator)\n\n    print(\"create trainer\")\n\n    # todo: reconcile logics\n    trainer = pl.Trainer(\n        max_steps=MAX_STEPS,\n        log_every_n_steps=1,\n        enable_progress_bar=True,\n        enable_model_summary=True,\n        val_check_interval=STEPS_PER_EPOCH,\n        check_val_every_n_epoch=None,  # because you're using step-based validation\n        gradient_clip_val=None,  # set if needed to prevent NaNs\n        accumulate_grad_batches=1,\n        precision=32,  # use 16 if fp16 mixed precision is desired\n        default_root_dir=\"output_checkpoints\",\n        enable_checkpointing=True,\n        callbacks=[\n            pl.callbacks.ModelCheckpoint(\n                dirpath=\"output_checkpoints\",\n                save_top_k=-1,\n                every_n_train_steps=500,\n                save_weights_only=False,\n                save_on_train_epoch_end=False,\n            ),\n            pl.callbacks.LearningRateMonitor(logging_interval='step'),\n        ],\n        logger=[\n            pl.loggers.TensorBoardLogger(save_dir=\"./tensorboard\", name=\"logs\"),\n            pl.loggers.WandbLogger(name=RUN_NAME, project=\"mqtl-classification\"),\n        ],\n        strategy=\"auto\",  # or \"ddp\" if using multiple GPUs manually\n        # gradient_checkpointing=True, # only available in huggingface trainer for better performance. not lightning ai\n    )\n\n    plModule = MQTLClassifierModule(mainModel)\n\n    try:\n        trainer.fit(plModule, train_dataloaders=train_loader, val_dataloaders=val_loader)\n    except Exception as x:\n        timber.error(f\"Error during training/evaluating: {x}\")\n    finally:\n        try:\n            trainer.test(plModule, dataloaders=test_loader)\n        except Exception as e:\n            timber.error(f\"Error during testing: {e}\")\n\n    save_fine_tuned_model(mainModel=mainModel)\n\n    pass\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T16:50:50.645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    # for some reason, the variables in the main function act like global variables in python\n    # hence other functions get confused with the \"global\" variables. easiest solution, write everything\n    # in another function (say, start()), and call it inside the main\n    start_time = datetime.now()\n\n    start()\n\n    end_time = datetime.now()\n    execution_time = end_time - start_time\n    total_seconds = execution_time.total_seconds()\n\n    # Convert total seconds into hours, minutes, and seconds\n    hours, remainder = divmod(total_seconds, 3600)\n    minutes, seconds = divmod(remainder, 60)\n\n    print(f\"Execution time: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n    pass\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T16:50:50.645Z"}},"outputs":[],"execution_count":null}]}