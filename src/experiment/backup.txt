
def createPagingTrainValTestDatasets(tokenizer, window, splitSequenceRequired) -> (PagingMQTLDataset, PagingMQTLDataset, PagingMQTLDataset):
    prefix = "/home/gamegame/PycharmProjects/mqtl-classification/"
    data_files = {
        # small samples
        "train_binned_200": f"{prefix}src/datageneration/dataset_200_train_binned.csv",
        "validate_binned_200": f"{prefix}src/datageneration/dataset_200_validate_binned.csv",
        "test_binned_200": f"{prefix}src/datageneration/dataset_200_test_binned.csv",
        # medium samples
        "train_binned_1000": f"{prefix}src/datageneration/dataset_1000_train_binned.csv",
        "validate_binned_1000": f"{prefix}src/datageneration/dataset_1000_train_binned.csv",
        "test_binned_1000": f"{prefix}src/datageneration/dataset_1000_train_binned.csv",

        # large samples
        "train_binned_4000": f"{prefix}src/datageneration/dataset_4000_train_binned.csv",
        "validate_binned_4000": f"{prefix}src/datageneration/dataset_4000_train_binned.csv",
        "test_binned_4000": f"{prefix}src/datageneration/dataset_4000_train_binned.csv",
    }

    dataset_map = None
    is_my_laptop = isMyLaptop()
    if is_my_laptop:
        dataset_map = load_dataset("csv", data_files=data_files, streaming=True)
    else:
        dataset_map = load_dataset("fahimfarhan/mqtl-classification-datasets", streaming=True)

    train_dataset = PagingMQTLDataset(someDataset=dataset_map[f"train_binned_{window}"],
                                    bertTokenizer=tokenizer,
                                    seqLength=window,
                                    splitSequenceRequired=splitSequenceRequired
                                    )
    val_dataset = PagingMQTLDataset(dataset_map[f"validate_binned_{window}"],
                                  bertTokenizer=tokenizer,
                                  seqLength=window,
                                  splitSequenceRequired=splitSequenceRequired
                                  )
    test_dataset = PagingMQTLDataset(dataset_map[f"test_binned_{window}"],
                                   bertTokenizer=tokenizer,
                                   seqLength=window,
                                   splitSequenceRequired=splitSequenceRequired
                                   )
    return train_dataset, val_dataset, test_dataset



class MqtlTrainer(Trainer):
    """
    Extending trainer to detect 1. anomalies, and 2. clip the gradients
    for debugging purposes. if the default trainer works, stick to defaults...
    """

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        torch.autograd.set_detect_anomaly(True)
        return super().compute_loss(model, inputs, return_outputs, num_items_in_batch)

    # def training_step(self, model, inputs, num_items_in_batch=None):
    #     model.train()
    #     inputs = self._prepare_inputs(inputs)
    #     with self.compute_loss_context_manager():
    #         loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
    #     loss.backward()
    #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # <=== gradient clipping
    #     return loss.detach()



# for hyenaDna. its tokenizer can process longer sequences...
def sequenceEncodePlusForHyenaDna(
    tokenizer: BertTokenizer,
    sequence: str,
    label: int
) -> BatchEncoding:
    tokenizedMap = tokenizer(sequence)
    input_ids = tokenizedMap["input_ids"]
    input_ids: torch.Tensor = torch.Tensor(input_ids)

    label_tensor = torch.tensor(label)
    encoded_map: dict = {
        "input_ids": input_ids.long(),
        # "attention_mask": attention_mask.int(),    # hyenaDNA does not have attention layer
        "labels": label_tensor
    }

    # Conditionally add attention_mask if it exists
    attention_mask = tokenizedMap.get("attention_mask", None)
    if attention_mask is not None:
        attention_mask = torch.Tensor(attention_mask)
        encoded_map["attention_mask"] = attention_mask.int()

    batchEncodingDict: BatchEncoding = BatchEncoding(encoded_map)
    return batchEncodingDict


# for dnaBert. it cannot process longer sequences...
def sequenceEncodePlusForDnaBert6(
        tokenizer: BertTokenizer,
        seq: str,
        label: int
) -> BatchEncoding:
    max_size = 512
    kmerSeq = toKmerSequence(seq, k=6)

    tempMap: BatchEncoding = tokenizer.encode_plus(
        kmerSeq,
        add_special_tokens=False,  # we'll add the special tokens manually in the for loop below
        return_attention_mask=True,
        return_tensors="pt"
    )

    someInputIds1xN = tempMap["input_ids"]  # shape = 1xN , N = sequence length
    someMasks1xN = tempMap["attention_mask"]
    inputIdsList = list(someInputIds1xN[0].split(510))
    masksList = list(someMasks1xN[0].split(510))

    tmpLength: int = len(inputIdsList)

    for i in range(0, tmpLength):
        cls: torch.Tensor = torch.Tensor([101])
        sep: torch.Tensor = torch.Tensor([102])

        isTokenUnitTensor = torch.Tensor([1])

        inputIdsList[i]: torch.Tensor = torch.cat([
            cls,
            inputIdsList[i],
            sep
        ])

        masksList[i] = torch.cat([
            isTokenUnitTensor,
            masksList[i],
            isTokenUnitTensor
        ])


        pad_len: int = max_size - inputIdsList[i].shape[0]
        if pad_len > 0:
            pad: torch.Tensor = torch.Tensor([0] * pad_len)

            inputIdsList[i]: torch.Tensor = torch.cat([
                inputIdsList[i],
                pad
            ])

            masksList[i]: torch.Tensor = torch.cat([
                masksList[i],
                pad
            ])


    # so each item len = 512, and the last one may have some padding
    input_ids: torch.Tensor = torch.stack(inputIdsList).squeeze()  # what's with this squeeze / unsqueeze thing? o.O
    attention_mask: torch.Tensor = torch.stack(masksList)
    label_tensor = torch.tensor(label)

    # print(f"{input_ids.shape = }")

    encoded_map: dict = {
        "input_ids": input_ids.long(),
        "attention_mask": attention_mask.int(),
        "labels": label_tensor
    }

    batchEncodingDict: BatchEncoding = BatchEncoding(encoded_map)
    return batchEncodingDict

def sequenceEncodePlusCompact(
        splitSequence: bool,
        tokenizer: BertTokenizer,
        sequence: str,
        label: int
) -> BatchEncoding:
    if splitSequence:
        return sequenceEncodePlusForDnaBert6(tokenizer, sequence, label)
    else:
        return sequenceEncodePlusForHyenaDna(tokenizer, sequence, label)